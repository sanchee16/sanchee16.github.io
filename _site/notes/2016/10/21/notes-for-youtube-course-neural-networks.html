<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title> Notes for Artificial Neural Networks! — A Course by Hugo Larochelle &raquo;  Sancheeta Kaushal</title>
<meta name="description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
">
<meta name="keywords" content="">
<link rel="canonical" href="http://hitchhiker.ma/notes/2016/10/21/notes-for-youtube-course-neural-networks.html">
        




<!-- Twitter Cards -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Notes for Artificial Neural Networks!" />
<meta name="twitter:description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
" />
<meta name="twitter:image" content="http://hitchhiker.ma" />

<!-- Google plus -->
<meta name="author" content="https://www.google.com/+MotaquillahMaddane">
<link rel="author" href="https://www.google.com/+MotaquillahMaddane">

<!-- Open Graph -->
<meta property="og:locale" content="">
<meta property="og:type" content="article">
<meta property="og:title" content="Notes for Artificial Neural Networks!">
<meta property="og:description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
">
<meta property="og:url" content="http://hitchhiker.ma/notes/2016/10/21/notes-for-youtube-course-neural-networks.html">
<meta property="og:site_name" content="Sancheeta Kaushal">

        <link href='http://fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/assets/vendor/normalize-css/normalize.css">
<link rel="stylesheet" href="/css/main.css">

  <link rel="stylesheet" href="/assets/vendor/highlight/styles/solarized_dark.css">

<link rel="stylesheet" href="/assets/vendor/font-awesome/css/font-awesome.css">
    </head>

    <body>
        <div class="wrapper">
            <header class="header">
    <div class="navigation">
        <a href="/" class="logo">Sancheeta Kaushal</a>

        <ul class="menu">
            <li class="menu__entry"><a href="/about">About</a></li>
            <li class="menu__entry"><a href="/">Blog</a></li>
            <li class="menu__entry"><a href="https://drive.google.com/open?id=0B00_UkobfItZXzFXTm5vUEV2MWc" target='_blank'>Resume</a></li>
        </ul>
    </div>

    <ul class="social-links">
        
            <a href="https://github.com/sanchee16" class="social-links__entry" target="_blank">
                <i class="fa fa-github"></i>
            </a>
        

        
            <a href="https://twitter.com/sanchee15" class="social-links__entry" target="_blank">
                <i class="fa fa-twitter"></i>
            </a>
        
    </ul>
</header>

            <h1 class="page-title post-title">
    <div class="page-title__text post-title__text">Notes for Artificial Neural Networks!</div>
    <div class="page-title__subtitle post-title__subtitle">A Course by Hugo Larochelle</div>
</h1>

<div class="content">
    <p>Lesson 1:</p>

<ul>
  <li>He defines the terminologies as weights denoted by w, bias denoted by b, activation function denoted by g(.), neuron preactivation(I/P) denoted by x, neuron activation(O/P) denoted by h(x). Everything exept b is a vector.</li>
  <li>He starts a discussion about potential activation functions in ANN.
    <ul>
      <li>Linear Activation Function, g(a) = a, No squashing of I/P
 	- Sigmoid Function, g(a) = 1/(1+e^(-a)), Lies between 0 and 1, always +ve, bounded, strictly increasing.
 	- Tanh Function, g(a) = (e^(2a)-1)/(e^(2a)+1), Lies netween -1 and 1, bounded, strictly increasing
 	- ReLU Function, g(a) = max(0, a), Bounded below 0, Strictly increasing, tends to give neurons with sparse activities because it is 0 over a large range of -ve numbers.</li>
    </ul>
  </li>
  <li>He further discusses the complexity of computations the neural network can perform.</li>
  <li>He talks about linearly separable problems(binary classification problems) which have a decision boundary and use single neuron to perform logistic regression. This can be achieved using sigmoid.</li>
  <li>He goes further and talks about alternate representations of input vector which can convert the non lineraly separable problem into a separable one. For eg. XOR can be represented by AND(X^, Y) on x axis and AND(X, Y^) on y axis. He tries to break the problem into pieces and check if single piece of problem can be represented by a neuron or a group of neurons and then combine all the representations to generate a representation for non linearly separable problem.</li>
  <li>He discusses the hidden layer pre-activation, hidden layer activation and output layer activation for a single layer neural network. Going ahead with multilayer neural network, the only difference is that multilayer neural network has multiple hidden layers.
    <ul>
      <li>To perform binary classification, one can use sigmoid as output activation function.</li>
      <li>For multiclass classification one needs multiple outputs, so one uses conditional probablity 
  p(y = c|x) where c is the class and then using softmax activation function; A function which sums to 1 when all the probablities are added. Additionally, softmax is strictly positive.</li>
    </ul>
  </li>
  <li>He discusses universal approximation theorm i.e. a single layer hidden neural network with linear output unit which can approx. any continuous function arbitrarily well, given enough hidden units.</li>
  <li>He discusses the inspiration that neural networks draw from biological neurons and talks about the idea of different layers and how they work in parallel with the visual cortex. He uses edges and points to introduce the idea of complex shapes formed using these simple structures and uses face detection as an example. He defines the following terms too.
    <ul>
      <li>Action potential is an electrical impulse that travels through the axon.</li>
      <li>Firing rate of a neuron is frequency with which a neuron can spike.</li>
      <li>Neurons tend to excite or inhibit other neurons.</li>
    </ul>
  </li>
  <li>Analogy between artificial NN and biological NN
    <ul>
      <li>Activation function is analogous to firing rate.</li>
      <li>Weights in ANN decide the excitation or inhibition of a neuron.</li>
      <li>Activation function and bias model the action potential of a neuron.</li>
    </ul>
  </li>
</ul>

<p>Lesson 2:</p>

<ul>
  <li>He starts with discussion on empirical risk minimization also called structural risk minimization 
 when using regularization.</li>
  <li>
    <script type="math/tex; mode=display">\arg\max_{\theta} \frac{1}{T} \sum_{t} l(f(x^t; \theta), y^t) + \lambda \Omega (\theta)</script>
    <ul>
      <li><script type="math/tex">\Omega (\theta)</script> is a regularizer</li>
      <li><script type="math/tex">l(f(x^t; \theta), y^t)</script> is a loss function</li>
    </ul>
  </li>
  <li>Learning hence becomes an optimization problem</li>
  <li>Ideally should optimize classification error but it’s not smooth as it’s either 0 or 1 so the above loss function is a surrogate for what we should truly optimise</li>
  <li>Stochastic Gradient Descent
    <ul>
      <li>Initialization of <script type="math/tex">\theta</script> i.e. weights and biases.</li>
      <li>for N interations
        <ul>
          <li>for each training example
            <ul>
              <li>figure out a direction for updating the parameters. Take the negative gradient of loss and regularizer</li>
              <li>Use the above gradient value along with <script type="math/tex">\alpha</script> ie learning rate to update the parameters</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Training epoch is equal to iteration over all the examples</li>
  <li>Neural network estimates <script type="math/tex">f(x)_c = p(y = c \lvert x)</script> for classification task</li>
  <li>Aim is to Maximize probability of <script type="math/tex">y^{(t)} \ given \ x^{(t)}</script> so we minimize - log likelihood 
 which is <script type="math/tex">l(f(x), y) = - log \ f(x)_{y}</script> also called as cross entropy</li>
</ul>

</div>

<div class="about">
    <div class="about__devider">*****</div>
    <div class="about__text">
        Written by <strong>  Sancheeta Kaushal </strong>
        on <strong>21 October 2016</strong>
    </div>
</div>


        </div>

        <script src="/assets/vendor/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
        
    </body>
</html>