<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sancheeta Kaushal</title>
    <description>Strong convictions precede great actions.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 20 Feb 2018 14:50:14 +0530</pubDate>
    <lastBuildDate>Tue, 20 Feb 2018 14:50:14 +0530</lastBuildDate>
    <generator>Jekyll v3.5.2</generator>
    
      <item>
        <title>Brain and behaviour - Regulating body weight!</title>
        <description>&lt;p&gt;Week 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Energy in = Energy out&lt;/li&gt;
  &lt;li&gt;Energy out
    &lt;ul&gt;
      &lt;li&gt;Physical Exercise&lt;/li&gt;
      &lt;li&gt;Basal Matabolic Rate&lt;/li&gt;
      &lt;li&gt;Diet-Induced Thermogenesis&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;5 meals a day&lt;/li&gt;
  &lt;li&gt;Food triggers
    &lt;ul&gt;
      &lt;li&gt;People with you&lt;/li&gt;
      &lt;li&gt;Visual Cues&lt;/li&gt;
      &lt;li&gt;Time of the day&lt;/li&gt;
      &lt;li&gt;Comfort foods&lt;/li&gt;
      &lt;li&gt;Learned Cues&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Homestasis refers to biological processes that keep certain body variables within a fixed range.
    &lt;ul&gt;
      &lt;li&gt;System variable is the variable that is controlled by a regulatory mechanism.&lt;/li&gt;
      &lt;li&gt;Set point is the optimal value of the system variable in the regulatory system.&lt;/li&gt;
      &lt;li&gt;Detector monitors the value of the system variable.&lt;/li&gt;
      &lt;li&gt;Correctional mechanism is required to restore the system to the set point.&lt;/li&gt;
      &lt;li&gt;Negative feedback is a process to diminish or terminate the action set in place to return the system variable to set point once set point is achieved. This is essential to regulatory systems.&lt;/li&gt;
      &lt;li&gt;Satiety mechanisms are produced by adequate and available supplies of the variable in question.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pavlov Dog Saliva Study&lt;/li&gt;
  &lt;li&gt;Classical or Pavlovian Conditioning&lt;/li&gt;
  &lt;li&gt;Unconditioned and Conditioned Response and Stimulus&lt;/li&gt;
  &lt;li&gt;Insulin increases in anticipation of food called cephalic phase insulin release&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Week 2:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Signals
    &lt;ul&gt;
      &lt;li&gt;Short Term signals tell about a meal&lt;/li&gt;
      &lt;li&gt;Long Term (Adiposity) arise from adipose/fat(leptin) tissue and pancreas(insulin)&lt;/li&gt;
      &lt;li&gt;CNS&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Blood Brain Barrier is the highly selective semi permeable membrane that protects the brain
    &lt;ul&gt;
      &lt;li&gt;Passive diffusion in fat soluble harmones, water&lt;/li&gt;
      &lt;li&gt;Active transport for leptin&lt;/li&gt;
      &lt;li&gt;Neural Transmission via vegus nerve(cranial nerve)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Orexigenic Appetite Stimulant and antonym anorexia is an eating disorder&lt;/li&gt;
  &lt;li&gt;Obesity depends on
    &lt;ul&gt;
      &lt;li&gt;How big is a meal&lt;/li&gt;
      &lt;li&gt;How much time between meals&lt;/li&gt;
      &lt;li&gt;How much time is spent eating a meal&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Roles of insulin
    &lt;ul&gt;
      &lt;li&gt;Involved in Short and Long term control of food intake and body weight regulation&lt;/li&gt;
      &lt;li&gt;Used in creating drugs for type 2 diabetes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hormones
    &lt;ul&gt;
      &lt;li&gt;Gut Hormones
        &lt;ul&gt;
          &lt;li&gt;Ghrelin
            &lt;ul&gt;
              &lt;li&gt;Increases food intake&lt;/li&gt;
              &lt;li&gt;Meal initiating hormone&lt;/li&gt;
              &lt;li&gt;Peripheral Hormone made in stomach&lt;/li&gt;
              &lt;li&gt;Acts at receptors in hypothalamus&lt;/li&gt;
              &lt;li&gt;Stimulates neurogenic peptides&lt;/li&gt;
              &lt;li&gt;Released in anticipation of eating food&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Cholecystokinin (CCK)
            &lt;ul&gt;
              &lt;li&gt;Released in intestinal tract and found in gut and brain&lt;/li&gt;
              &lt;li&gt;Tells body and brain when and what food is ingested&lt;/li&gt;
              &lt;li&gt;Reduces food intake&lt;/li&gt;
              &lt;li&gt;Found in duodenum and jejunum ie upper and middle small intestine&lt;/li&gt;
              &lt;li&gt;Trianguar shaped and apical side is towards the inside of intestine&lt;/li&gt;
              &lt;li&gt;Basal Plasma CCK levels increase with a meal&lt;/li&gt;
              &lt;li&gt;Dietary Fat and protein are more potent stimulators of CCK than carbohydrates&lt;/li&gt;
              &lt;li&gt;Obesity not treated using this as it reduces the meal size but increases the frequency of the meals in a day&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Glucagon-like Peptide 1
            &lt;ul&gt;
              &lt;li&gt;Produced in intestinal tract ie ileum and closer to the stomach&lt;/li&gt;
              &lt;li&gt;Also produced in hindbrain (Nucleus of solitary tract)&lt;/li&gt;
              &lt;li&gt;Stops food intake&lt;/li&gt;
              &lt;li&gt;During fast period, concentrations of GLP-1 are low&lt;/li&gt;
              &lt;li&gt;Incretin hormone amplify insulin secretion&lt;/li&gt;
              &lt;li&gt;Used to model drugs for diabetes&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 20 Feb 2018 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2018/02/notes-for-brain-and-behaviour-regulating-body-weight/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/notes-for-brain-and-behaviour-regulating-body-weight/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Good Brain Bad Brain!</title>
        <description>&lt;p&gt;Week 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Gyrus are the parts of the brain visible on the surface between the grooves&lt;/li&gt;
  &lt;li&gt;The bag that keeps the chemicals together is known as the cell membrane and the chemicals themselves are all that is necessary for that cell to be alive.&lt;/li&gt;
  &lt;li&gt;According to estimates, humans contain 3.72 × 10^13 cells.&lt;/li&gt;
  &lt;li&gt;Endocrine system releases harmones.&lt;/li&gt;
  &lt;li&gt;Cell control = Endocrine + Nervous System&lt;/li&gt;
  &lt;li&gt;Anatomical division of Nervous System
    &lt;ul&gt;
      &lt;li&gt;CNS (Central)
        &lt;ul&gt;
          &lt;li&gt;Brain&lt;/li&gt;
          &lt;li&gt;Spinal Cord&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;PNS (Peripheral)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Brain
    &lt;ul&gt;
      &lt;li&gt;Lateral Sulcus&lt;/li&gt;
      &lt;li&gt;Cental Sulcus&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Based on Sulcuses, we have four parts
    &lt;ul&gt;
      &lt;li&gt;Frontal Lobe&lt;/li&gt;
      &lt;li&gt;Temporal Lobe&lt;/li&gt;
      &lt;li&gt;Parietal Lobe&lt;/li&gt;
      &lt;li&gt;Occipital Lobe&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Two types of brain cells
    &lt;ul&gt;
      &lt;li&gt;Neurons are information processors and transmitters&lt;/li&gt;
      &lt;li&gt;Glia cells provide physical support for neurons and may have specific functions too.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Neurons
    &lt;ul&gt;
      &lt;li&gt;Usually Colourless&lt;/li&gt;
      &lt;li&gt;Smaller than most cells&lt;/li&gt;
      &lt;li&gt;Different structure than most cells
        &lt;ul&gt;
          &lt;li&gt;Dendrites&lt;/li&gt;
          &lt;li&gt;Axon&lt;/li&gt;
          &lt;li&gt;Ternimals/Synaptic ends&lt;/li&gt;
          &lt;li&gt;Myelin - type of glia cells which aid the process in axon&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Types of neurons
    &lt;ul&gt;
      &lt;li&gt;Sensory&lt;/li&gt;
      &lt;li&gt;Motor&lt;/li&gt;
      &lt;li&gt;Interneurons&lt;/li&gt;
      &lt;li&gt;Neurons in the brain&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Maximum number of neurons as toddlers&lt;/li&gt;
  &lt;li&gt;Limited areas have neurogenesis properties in adulthood&lt;/li&gt;
  &lt;li&gt;Darker Areas
    &lt;ul&gt;
      &lt;li&gt;Cell bodies and dendrites&lt;/li&gt;
      &lt;li&gt;Gray Matter&lt;/li&gt;
      &lt;li&gt;Found on outer surface and center of brain&lt;/li&gt;
      &lt;li&gt;Folds on the cortical surface are also lined with grey matter and therefore the folds simply act to increase the surface area&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Lighter Areas
    &lt;ul&gt;
      &lt;li&gt;Axons&lt;/li&gt;
      &lt;li&gt;White Matter&lt;/li&gt;
      &lt;li&gt;Found beneath the gray matter and form bridge between two halves of brain&lt;/li&gt;
      &lt;li&gt;The myelin produced by the oligodendrocytes is a type of lipid (fat) and has a pale appearance so the color of white matter&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Glial Cell Type
    &lt;ul&gt;
      &lt;li&gt;Oligodendrocyte type in CNS&lt;/li&gt;
      &lt;li&gt;Schwann cell type in PNS&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Why the folded structure
    &lt;ul&gt;
      &lt;li&gt;As brain evolved to grow bigger and contain more cells there were two options&lt;/li&gt;
      &lt;li&gt;Increase surface area or folded structure&lt;/li&gt;
      &lt;li&gt;Rodents have smooth brains&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Brain function is localised&lt;/li&gt;
  &lt;li&gt;Similar looking neurons are grouped together and have same functions&lt;/li&gt;
  &lt;li&gt;Penfield’s maps&lt;/li&gt;
  &lt;li&gt;Homunculus is neurological map of human body for motor functions&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 15 Feb 2018 15:53:34 +0530</pubDate>
        <link>http://localhost:4000/2018/02/notes-for-good-brain-bad-brain/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/notes-for-good-brain-bad-brain/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Fundamentals of Neuroscience!</title>
        <description>&lt;p&gt;Course 0:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Guided Interaction&lt;/li&gt;
  &lt;li&gt;Key to human brain is not the size but the inner wiring, it’s structure and function&lt;/li&gt;
  &lt;li&gt;Nervous System
    &lt;ul&gt;
      &lt;li&gt;CNS (Central Nervous System) contains brain and spinal cord&lt;/li&gt;
      &lt;li&gt;PNS (Peripheral Nervous System) contains peripheral nervous&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Neurons&lt;/li&gt;
  &lt;li&gt;Complexity of brains&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Course 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Electricity&lt;/li&gt;
  &lt;li&gt;Galvani and bioelectricity&lt;/li&gt;
  &lt;li&gt;Volta and artificial electricity&lt;/li&gt;
  &lt;li&gt;Neuron parts
    &lt;ul&gt;
      &lt;li&gt;Cell body or Soma
        &lt;ul&gt;
          &lt;li&gt;Membrane Potential (-70 mV between inside and outside of membrane)&lt;/li&gt;
          &lt;li&gt;Outside part is considered Ground&lt;/li&gt;
          &lt;li&gt;Water, Proteins, Ions and Sugars&lt;/li&gt;
          &lt;li&gt;Key Ions (Na, Ca, K, Cl)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Axons and Dendrites&lt;/li&gt;
      &lt;li&gt;Lipid Bilayer (Cell Membrane)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Resting Potential
    &lt;ul&gt;
      &lt;li&gt;Diffusion (Movement from regions of high concentration to low concentration)&lt;/li&gt;
      &lt;li&gt;Electrostatic force&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 02 Feb 2018 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2018/02/notes-for-fundamentals-of-neuroscience/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/notes-for-fundamentals-of-neuroscience/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Understanding the Brain - The Neurobiology of Everyday Life!</title>
        <description>&lt;p&gt;Week 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Locked-in Syndrome&lt;/li&gt;
  &lt;li&gt;Diving Bell and the Butterfly&lt;/li&gt;
  &lt;li&gt;Four functions
    &lt;ul&gt;
      &lt;li&gt;Voluntary Movement
        &lt;ul&gt;
          &lt;li&gt;MotorNeurons&lt;/li&gt;
          &lt;li&gt;Brain Stem or Spinal Cord&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Perception
        &lt;ul&gt;
          &lt;li&gt;Awareness of a Sensation&lt;/li&gt;
          &lt;li&gt;Forebrain -&amp;gt; Cerebral Cortex&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Homeostasis
        &lt;ul&gt;
          &lt;li&gt;Process of maintaining of healthy internal conditions&lt;/li&gt;
          &lt;li&gt;Forebrain(Hormonal Changes), Brain Stem and Spinal Cord&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Abstract Functions
        &lt;ul&gt;
          &lt;li&gt;Emotions, Thinking, Language, Memory&lt;/li&gt;
          &lt;li&gt;Forebrain&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Brain Stem (Hind and Mid Brain) -&amp;gt; Cerebellum&lt;/li&gt;
  &lt;li&gt;Foramen Magnum hole in the base of Skull from which Spinal Cord passes&lt;/li&gt;
  &lt;li&gt;Neurons
    &lt;ul&gt;
      &lt;li&gt;Unique as they are longest cells in the body&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Parts of Neurons
    &lt;ul&gt;
      &lt;li&gt;Cell body/Soma contains Neucleous&lt;/li&gt;
      &lt;li&gt;Dendrites (Dendritic Tree/Dendritic Arbor)&lt;/li&gt;
      &lt;li&gt;Axon (Synaptic Terminal at the end)&lt;/li&gt;
      &lt;li&gt;Synapse&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 23 Jan 2018 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2018/01/notes-for-understanding-the-brain/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/notes-for-understanding-the-brain/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Deep Learning fast.ai MOOC!</title>
        <description>&lt;p&gt;Lesson 0:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Incredibly flexible function (NN), all-purpose parameter fitting(Backprop), fast and scalable(GPU) made DL possible&lt;/li&gt;
  &lt;li&gt;?? before any function describes the function in Ipynb&lt;/li&gt;
  &lt;li&gt;Shift + Enter in Ipynb&lt;/li&gt;
  &lt;li&gt;Try things with your own data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Infinitely flexible function, all purpose parameter fitting and tuning which is fast and scalable.&lt;/li&gt;
  &lt;li&gt;Neural network is the universal approximation function.&lt;/li&gt;
  &lt;li&gt;Gradient Descent &amp;amp; Backpropagation is the tuning part.&lt;/li&gt;
  &lt;li&gt;NVIDIA GPU supports CUDA which is efficient in running deep learning computations.&lt;/li&gt;
  &lt;li&gt;P2 and t2.micro&lt;/li&gt;
  &lt;li&gt;AWS Setup&lt;/li&gt;
  &lt;li&gt;AMI: Amazon Machine Images: Snapshot of instance at a given point in time&lt;/li&gt;
  &lt;li&gt;Literate programming&lt;/li&gt;
  &lt;li&gt;Cmd + Shift + P for keyboard shortcuts on jupyter notebook&lt;/li&gt;
  &lt;li&gt;tmux&lt;/li&gt;
  &lt;li&gt;Test, Train and Validation Datasets. Also, introduced to sample data.&lt;/li&gt;
  &lt;li&gt;Magic function&lt;/li&gt;
  &lt;li&gt;Pretrained Model - Model with Learned Parameters&lt;/li&gt;
  &lt;li&gt;VGG 16 model&lt;/li&gt;
  &lt;li&gt;Keras.json &amp;amp; theanorc file for configuration&lt;/li&gt;
  &lt;li&gt;Concept of Batch and Mini Batch&lt;/li&gt;
  &lt;li&gt;GPU has limited memory and the data transfer is a costly operation&lt;/li&gt;
  &lt;li&gt;Finetuning replaces the last layer of a pretrained network with the current classes&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 28 Dec 2017 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2017/12/notes-for-deep-learning-fast-ai/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/12/notes-for-deep-learning-fast-ai/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Design Patterns</title>
        <description>&lt;p&gt;Lesson 0:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Experience Reuse&lt;/li&gt;
  &lt;li&gt;Obeserver Pattern&lt;/li&gt;
  &lt;li&gt;Decorator Pattern&lt;/li&gt;
  &lt;li&gt;Factory Pattern&lt;/li&gt;
  &lt;li&gt;Singleton Pattern&lt;/li&gt;
  &lt;li&gt;Command Pattern&lt;/li&gt;
  &lt;li&gt;Adapter and Facade Pattern&lt;/li&gt;
  &lt;li&gt;Template Method Pattern&lt;/li&gt;
  &lt;li&gt;Iterator and Composite Pattern&lt;/li&gt;
  &lt;li&gt;State Pattern&lt;/li&gt;
  &lt;li&gt;Proxy Pattern&lt;/li&gt;
  &lt;li&gt;Compound Pattern&lt;/li&gt;
  &lt;li&gt;Design Pattern&lt;/li&gt;
  &lt;li&gt;Leftover Pattern&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 1:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Experience reuse&lt;/li&gt;
  &lt;li&gt;Change is the only constant&lt;/li&gt;
  &lt;li&gt;Exercise answers
    &lt;ul&gt;
      &lt;li&gt;Need to improve code for scalabitlity&lt;/li&gt;
      &lt;li&gt;Prototyping for experiments and then building products if the prototype is successful&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Design Principles
    &lt;ul&gt;
      &lt;li&gt;Identify aspects of app which vary and encapsulate them&lt;/li&gt;
      &lt;li&gt;Program to an interface not an implementation&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Behavious Class&lt;/li&gt;
  &lt;li&gt;Interface&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 18 Dec 2017 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2017/12/notes-for-head-first-design-patterns/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/12/notes-for-head-first-design-patterns/</guid>
        
        <category>Book</category>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for CNN for Visual Recognition!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Today is age of images/video but image/video data is hard to use&lt;/li&gt;
  &lt;li&gt;Computer Vision as an interdisciplinary field&lt;/li&gt;
  &lt;li&gt;Eye as an inspiration for big bang of species&lt;/li&gt;
  &lt;li&gt;Vision important for speciation in the early evolution of the species&lt;/li&gt;
  &lt;li&gt;Mechanical Vision i.e. Camera models - Camera Obscura&lt;/li&gt;
  &lt;li&gt;Hubel and Wiesel experiment&lt;/li&gt;
  &lt;li&gt;Primary visual cortex is very far away from eye&lt;/li&gt;
  &lt;li&gt;Simple structures excite neurons in human brain&lt;/li&gt;
  &lt;li&gt;Block World models - Visual world simplified into basic geometrical shapes&lt;/li&gt;
  &lt;li&gt;Book Vision by david marr&lt;/li&gt;
  &lt;li&gt;Hierarchical Representation&lt;/li&gt;
  &lt;li&gt;Generalized Cylinder and Pictorial Structure models&lt;/li&gt;
  &lt;li&gt;Normalized Cut - perceptual grouping problem - image segmentation&lt;/li&gt;
  &lt;li&gt;Real time Face Detection by Fuji Films Camera&lt;/li&gt;
  &lt;li&gt;Major focus is recognition&lt;/li&gt;
  &lt;li&gt;Engineered Features like SIFT, HOG&lt;/li&gt;
  &lt;li&gt;Spatial pyramid matching for scene recognition&lt;/li&gt;
  &lt;li&gt;Deformable Part Model&lt;/li&gt;
  &lt;li&gt;PASCAL visual object challenge&lt;/li&gt;
  &lt;li&gt;Imagenet&lt;/li&gt;
  &lt;li&gt;Image classification really useful for making progress in other image machine learning problems like image detection, segmentation, image captioning&lt;/li&gt;
  &lt;li&gt;Semantic Segmentation and Perceptual Grouping&lt;/li&gt;
  &lt;li&gt;Tell a story given a scene&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 2:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Image Classification&lt;/li&gt;
  &lt;li&gt;Semantic Gap - Representation of image on computer as numbers&lt;/li&gt;
  &lt;li&gt;Challenges are:
    &lt;ul&gt;
      &lt;li&gt;Viewpoint Variation - Camera rotations lead to changes in brightness.&lt;/li&gt;
      &lt;li&gt;Illumination issues&lt;/li&gt;
      &lt;li&gt;Deformation&lt;/li&gt;
      &lt;li&gt;Occlusion&lt;/li&gt;
      &lt;li&gt;Background Clutter&lt;/li&gt;
      &lt;li&gt;Intraclass Variation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Follow a data-driven approach i.e. Collect dataset of images and labels, use ML algos to train an image 
classifier and evaluate classifier on test images.&lt;/li&gt;
  &lt;li&gt;Nearest Neighbour Classifier - Use Manhattan distance&lt;/li&gt;
  &lt;li&gt;Do more compute at train time but prediction should be constant time computation.&lt;/li&gt;
  &lt;li&gt;Aside - Approximate Nearest Neighbour (FLANN)&lt;/li&gt;
  &lt;li&gt;Hyper-parameter - Distance, and the value of k for kNN&lt;/li&gt;
  &lt;li&gt;Training data, validation data and training data set or Cross Validation&lt;/li&gt;
  &lt;li&gt;Linear Classification&lt;/li&gt;
  &lt;li&gt;NN can see, hear, translate, control and think.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lecture 4:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Forward pass gives loss and backward pass gives gradients&lt;/li&gt;
  &lt;li&gt;Gradient Descent
    &lt;ul&gt;
      &lt;li&gt;Numerical which is slow and approx but easy to write&lt;/li&gt;
      &lt;li&gt;Analytical which is fast and exact but error prone&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Computational Graph is huge for Neural Turing Machine&lt;/li&gt;
  &lt;li&gt;Chain rule for backprop&lt;/li&gt;
  &lt;li&gt;Local gradients are computed at the time of forward pass and can be chained to global gradient later at the time of backprop.&lt;/li&gt;
  &lt;li&gt;For plus gate, during back prop, the value for the next gate is 1 * the previous value.&lt;/li&gt;
  &lt;li&gt;For multiplicative gate, during back prop, the value for the next gate is the value of other input * the previous value.&lt;/li&gt;
  &lt;li&gt;Hence, add gate is gradient distributor, the max gate is gradient router and mul gate can be the gradient switcher.&lt;/li&gt;
  &lt;li&gt;At branches, gradients are added according to multivariate chain rule.&lt;/li&gt;
  &lt;li&gt;Graph class with nodes topologically sorted and forward and backward function&lt;/li&gt;
  &lt;li&gt;Lot of memory required to store intermediate results that will be used during back prop&lt;/li&gt;
  &lt;li&gt;For vectors, we have jacobian matrix which stores derivative of each element of output wrt input&lt;/li&gt;
  &lt;li&gt;Vectorized Operations&lt;/li&gt;
  &lt;li&gt;Jacobian matrix is not always a full matrix and is a sparse matrix because there are values only on the diagonal and even not all those values are be used.&lt;/li&gt;
  &lt;li&gt;Backpropagation is recursive application of chain rule along computational graph to computer gradients  of all inputs/params/intermediates&lt;/li&gt;
  &lt;li&gt;Biological description of neurons
    &lt;ul&gt;
      &lt;li&gt;Soma: cell body&lt;/li&gt;
      &lt;li&gt;Dendrites: listeners/input&lt;/li&gt;
      &lt;li&gt;Axon: terminals/output&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Activation functions
    &lt;ul&gt;
      &lt;li&gt;Sigmoid&lt;/li&gt;
      &lt;li&gt;tanh&lt;/li&gt;
      &lt;li&gt;ReLU&lt;/li&gt;
      &lt;li&gt;Maxout&lt;/li&gt;
      &lt;li&gt;Leaky ReLU&lt;/li&gt;
      &lt;li&gt;ELU&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Fully connected layer and hidden layers&lt;/li&gt;
  &lt;li&gt;Kernel trick changes data representation to a space where it’s linearly separable&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lecture 7:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CNN operate over volumes.&lt;/li&gt;
  &lt;li&gt;Filters are convolved over the image ie the filter is slid over the image spatially computing dot products which result in activation map.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\text{output_size} = ((N-F+2*P)/S) + 1&lt;/script&gt; where N is width/height, F is filter size, P is padding and S is stride.&lt;/li&gt;
  &lt;li&gt;Input padding is a common practice since we want to preserve sizes spatially otherwise the size of the input decreases sharply.&lt;/li&gt;
  &lt;li&gt;To always achieve same output volume spatially for stride of 1, use &lt;script type=&quot;math/tex&quot;&gt;(F-1)/2&lt;/script&gt; zero padding&lt;/li&gt;
  &lt;li&gt;K, N, F and P are hyperparameters where K is the number of filters and is in powers of 2 as certains subroutines are efficient in computations with a power of 2.&lt;/li&gt;
  &lt;li&gt;The depth of the output of a convolution will the total number of filters.&lt;/li&gt;
  &lt;li&gt;With parameter sharing it introduces, &lt;script type=&quot;math/tex&quot;&gt;F*F*D_1&lt;/script&gt; weights per filter.&lt;/li&gt;
  &lt;li&gt;1*1 convolutions are important since they return the same sized output since we do dot products over the full depth of the volume (depth columns or fibres).&lt;/li&gt;
  &lt;li&gt;The size of F is usually odd.&lt;/li&gt;
  &lt;li&gt;Usually, images are preprocessed to squares.&lt;/li&gt;
  &lt;li&gt;The filter is also called kernel. Filters capture local information.&lt;/li&gt;
  &lt;li&gt;Along the depth of the output volume, all the neurons have actually looked at the same patch but their weights will still be different.&lt;/li&gt;
  &lt;li&gt;Pooling layer makes the representations smaller and managable&lt;/li&gt;
  &lt;li&gt;Pooling operates over each activation map independently&lt;/li&gt;
  &lt;li&gt;LeNet - 5&lt;/li&gt;
  &lt;li&gt;AlexnNet
    &lt;ul&gt;
      &lt;li&gt;Use of ReLU&lt;/li&gt;
      &lt;li&gt;Used norm layers&lt;/li&gt;
      &lt;li&gt;Heavy data augmentation&lt;/li&gt;
      &lt;li&gt;Dropout&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ZFNet&lt;/li&gt;
  &lt;li&gt;VGGNet&lt;/li&gt;
  &lt;li&gt;GoogleNet&lt;/li&gt;
  &lt;li&gt;ResNet
    &lt;ul&gt;
      &lt;li&gt;Skip Step&lt;/li&gt;
      &lt;li&gt;Batch normalization layer and hence can use a higher learning rate&lt;/li&gt;
      &lt;li&gt;No dropout&lt;/li&gt;
      &lt;li&gt;Xavier/2 initialization&lt;/li&gt;
      &lt;li&gt;Faster than VGGNet(20 layers) inspite of having 152 layers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Policy network&lt;/li&gt;
  &lt;li&gt;As we go ahead with the architectures, we find that the numbers of parameters are reduced but more conputation is required and the results are too promising.&lt;/li&gt;
  &lt;li&gt;Instead of fully connected layers, use average pooling layers in the end of a CNN.&lt;/li&gt;
  &lt;li&gt;Convnets stack - CONV, POOL, Fully Connected layers&lt;/li&gt;
  &lt;li&gt;Trend towards smaller filters and deeper networks and getting rid of POOL/FC layers&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 09 Oct 2017 15:30:12 +0530</pubDate>
        <link>http://localhost:4000/2017/10/notes-for-CNN-for-visual-recognition/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/10/notes-for-CNN-for-visual-recognition/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Deep Learning STAT 946</title>
        <description>&lt;p&gt;Lesson 6:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Network with atleast one convolutional layer&lt;/li&gt;
  &lt;li&gt;Three stages
    &lt;ul&gt;
      &lt;li&gt;Convolution function (Discrete Convolution)&lt;/li&gt;
      &lt;li&gt;Detection Eg. ReLU&lt;/li&gt;
      &lt;li&gt;Pooling&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Convolution vs Cross Corelation (Replace + by - in convolution)&lt;/li&gt;
  &lt;li&gt;Convolved Feature or Feature Map - captures features in image similar to kernel&lt;/li&gt;
  &lt;li&gt;Sharing parameters&lt;/li&gt;
  &lt;li&gt;Making the network sparse&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 22 Jun 2017 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2017/06/notes-for-deep-learning-waterloo/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/06/notes-for-deep-learning-waterloo/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Artificial Neural Networks!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;He defines the terminologies as weights denoted by w, bias denoted by b, activation function denoted by g(.), neuron preactivation(I/P) denoted by x, neuron activation(O/P) denoted by h(x). Everything exept b is a vector.&lt;/li&gt;
  &lt;li&gt;He starts a discussion about potential activation functions in ANN.
    &lt;ul&gt;
      &lt;li&gt;Linear Activation Function, g(a) = a, No squashing of I/P
 	- Sigmoid Function, g(a) = 1/(1+e^(-a)), Lies between 0 and 1, always +ve, bounded, strictly increasing.
 	- Tanh Function, g(a) = (e^(2a)-1)/(e^(2a)+1), Lies netween -1 and 1, bounded, strictly increasing
 	- ReLU Function, g(a) = max(0, a), Bounded below 0, Strictly increasing, tends to give neurons with sparse activities because it is 0 over a large range of -ve numbers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;He further discusses the complexity of computations the neural network can perform.&lt;/li&gt;
  &lt;li&gt;He talks about linearly separable problems(binary classification problems) which have a decision boundary and use single neuron to perform logistic regression. This can be achieved using sigmoid.&lt;/li&gt;
  &lt;li&gt;He goes further and talks about alternate representations of input vector which can convert the non lineraly separable problem into a separable one. For eg. XOR can be represented by AND(X^, Y) on x axis and AND(X, Y^) on y axis. He tries to break the problem into pieces and check if single piece of problem can be represented by a neuron or a group of neurons and then combine all the representations to generate a representation for non linearly separable problem.&lt;/li&gt;
  &lt;li&gt;He discusses the hidden layer pre-activation, hidden layer activation and output layer activation for a single layer neural network. Going ahead with multilayer neural network, the only difference is that multilayer neural network has multiple hidden layers.
    &lt;ul&gt;
      &lt;li&gt;To perform binary classification, one can use sigmoid as output activation function.&lt;/li&gt;
      &lt;li&gt;For multiclass classification one needs multiple outputs, so one uses conditional probablity 
  p(y = c|x) where c is the class and then using softmax activation function; A function which sums to 1 when all the probablities are added. Additionally, softmax is strictly positive.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;He discusses universal approximation theorm i.e. a single layer hidden neural network with linear output unit which can approx. any continuous function arbitrarily well, given enough hidden units.&lt;/li&gt;
  &lt;li&gt;He discusses the inspiration that neural networks draw from biological neurons and talks about the idea of different layers and how they work in parallel with the visual cortex. He uses edges and points to introduce the idea of complex shapes formed using these simple structures and uses face detection as an example. He defines the following terms too.
    &lt;ul&gt;
      &lt;li&gt;Action potential is an electrical impulse that travels through the axon.&lt;/li&gt;
      &lt;li&gt;Firing rate of a neuron is frequency with which a neuron can spike.&lt;/li&gt;
      &lt;li&gt;Neurons tend to excite or inhibit other neurons.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Analogy between artificial NN and biological NN
    &lt;ul&gt;
      &lt;li&gt;Activation function is analogous to firing rate.&lt;/li&gt;
      &lt;li&gt;Weights in ANN decide the excitation or inhibition of a neuron.&lt;/li&gt;
      &lt;li&gt;Activation function and bias model the action potential of a neuron.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 2:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;He starts with discussion on empirical risk minimization also called structural risk minimization 
 when using regularization.&lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\arg\max_{\theta} \frac{1}{T} \sum_{t} l(f(x^t; \theta), y^t) + \lambda \Omega (\theta)&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\Omega (\theta)&lt;/script&gt; is a regularizer&lt;/li&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;l(f(x^t; \theta), y^t)&lt;/script&gt; is a loss function&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Learning hence becomes an optimization problem&lt;/li&gt;
  &lt;li&gt;Ideally should optimize classification error but it’s not smooth as it’s either 0 or 1 so the above loss function is a surrogate for what we should truly optimise&lt;/li&gt;
  &lt;li&gt;Stochastic Gradient Descent
    &lt;ul&gt;
      &lt;li&gt;Initialization of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; i.e. weights and biases.&lt;/li&gt;
      &lt;li&gt;for N interations
        &lt;ul&gt;
          &lt;li&gt;for each training example
            &lt;ul&gt;
              &lt;li&gt;figure out a direction for updating the parameters. Take the negative gradient of loss and regularizer&lt;/li&gt;
              &lt;li&gt;Use the above gradient value along with &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; ie learning rate to update the parameters&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Training epoch is equal to iteration over all the examples&lt;/li&gt;
  &lt;li&gt;Neural network estimates &lt;script type=&quot;math/tex&quot;&gt;f(x)_c = p(y = c \lvert x)&lt;/script&gt; for classification task&lt;/li&gt;
  &lt;li&gt;Aim is to Maximize probability of &lt;script type=&quot;math/tex&quot;&gt;y^{(t)} \ given \ x^{(t)}&lt;/script&gt; so we minimize - log likelihood 
 which is &lt;script type=&quot;math/tex&quot;&gt;l(f(x), y) = - log \ f(x)_{y}&lt;/script&gt; also called as cross entropy&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 18 Apr 2017 17:15:34 +0530</pubDate>
        <link>http://localhost:4000/2017/04/notes-for-youtube-course-neural-networks/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/04/notes-for-youtube-course-neural-networks/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Intro to Data Analysis!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data Analysis Process
    &lt;ul&gt;
      &lt;li&gt;Question&lt;/li&gt;
      &lt;li&gt;Wrangle
        &lt;ul&gt;
          &lt;li&gt;Acquiring Data&lt;/li&gt;
          &lt;li&gt;Cleaning Data&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Explore&lt;/li&gt;
      &lt;li&gt;Draw conclusions&lt;/li&gt;
      &lt;li&gt;Communicate&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The process is iterative and need to go back and forth.&lt;/li&gt;
  &lt;li&gt;Each row of csv can be represented in two ways in python
    &lt;ul&gt;
      &lt;li&gt;As a list&lt;/li&gt;
      &lt;li&gt;As a dictionary&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 24 Mar 2017 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2017/03/notes-for-udacity-intro-to-data-analysis/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/notes-for-udacity-intro-to-data-analysis/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
  </channel>
</rss>
