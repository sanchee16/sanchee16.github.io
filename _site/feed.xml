<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sancheeta Kaushal</title>
    <description>Strong convictions precede great actions.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 06 Feb 2018 09:43:36 +0530</pubDate>
    <lastBuildDate>Tue, 06 Feb 2018 09:43:36 +0530</lastBuildDate>
    <generator>Jekyll v3.5.2</generator>
    
      <item>
        <title>Fundamentals of Neuroscience!</title>
        <description>&lt;p&gt;Course 0:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Guided Interaction&lt;/li&gt;
  &lt;li&gt;Key to human brain is not the size but the inner wiring, itâ€™s structure and function&lt;/li&gt;
  &lt;li&gt;Nervous System
    &lt;ul&gt;
      &lt;li&gt;CNS (Central Nervous System) contains brain and spinal cord&lt;/li&gt;
      &lt;li&gt;PNS (Peripheral Nervous System) contains peripheral nervous&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Neurons&lt;/li&gt;
  &lt;li&gt;Complexity of brains&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Course 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Electricity&lt;/li&gt;
  &lt;li&gt;Galvani and bioelectricity&lt;/li&gt;
  &lt;li&gt;Volta and artificial electricity&lt;/li&gt;
  &lt;li&gt;Neuron parts
    &lt;ul&gt;
      &lt;li&gt;Cell body or Soma
        &lt;ul&gt;
          &lt;li&gt;Membrane Potential (-70 mV between inside and outside of membrane)&lt;/li&gt;
          &lt;li&gt;Outside part is considered Ground&lt;/li&gt;
          &lt;li&gt;Water, Proteins, Ions and Sugars&lt;/li&gt;
          &lt;li&gt;Key Ions (Na, Ca, K, Cl)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Axons and Dendrites&lt;/li&gt;
      &lt;li&gt;Lipid Bilayer (Cell Membrane)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Resting Potential
    &lt;ul&gt;
      &lt;li&gt;Diffusion (Movement from regions of high concentration to low concentration)&lt;/li&gt;
      &lt;li&gt;Electrostatic force&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 23 Jan 2018 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2018/01/notes-for-fundamentals-of-neuroscience/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/notes-for-fundamentals-of-neuroscience/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Understanding the Brain - The Neurobiology of Everyday Life!</title>
        <description>&lt;p&gt;Week 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Locked-in Syndrome&lt;/li&gt;
  &lt;li&gt;Diving Bell and the Butterfly&lt;/li&gt;
  &lt;li&gt;Four functions
    &lt;ul&gt;
      &lt;li&gt;Voluntary Movement
        &lt;ul&gt;
          &lt;li&gt;MotorNeurons&lt;/li&gt;
          &lt;li&gt;Brain Stem or Spinal Cord&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Perception
        &lt;ul&gt;
          &lt;li&gt;Awareness of a Sensation&lt;/li&gt;
          &lt;li&gt;Forebrain -&amp;gt; Cerebral Cortex&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Homeostasis
        &lt;ul&gt;
          &lt;li&gt;Process of maintaining of healthy internal conditions&lt;/li&gt;
          &lt;li&gt;Forebrain(Hormonal Changes), Brain Stem and Spinal Cord&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Abstract Functions
        &lt;ul&gt;
          &lt;li&gt;Emotions, Thinking, Language, Memory&lt;/li&gt;
          &lt;li&gt;Forebrain&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Brain Stem (Hind and Mid Brain) -&amp;gt; Cerebellum&lt;/li&gt;
  &lt;li&gt;Foramen Magnum hole in the base of Skull from which Spinal Cord passes&lt;/li&gt;
  &lt;li&gt;Neurons
    &lt;ul&gt;
      &lt;li&gt;Unique as they are longest cells in the body&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Parts of Neurons
    &lt;ul&gt;
      &lt;li&gt;Cell body/Soma contains Neucleous&lt;/li&gt;
      &lt;li&gt;Dendrites (Dendritic Tree/Dendritic Arbor)&lt;/li&gt;
      &lt;li&gt;Axon (Synaptic Terminal at the end)&lt;/li&gt;
      &lt;li&gt;Synapse&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 23 Jan 2018 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2018/01/notes-for-understanding-the-brain/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/notes-for-understanding-the-brain/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Deep Learning fast.ai MOOC!</title>
        <description>&lt;p&gt;Lesson 0:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Incredibly flexible function (NN), all-purpose parameter fitting(Backprop), fast and scalable(GPU) made DL possible&lt;/li&gt;
  &lt;li&gt;?? before any function describes the function in Ipynb&lt;/li&gt;
  &lt;li&gt;Shift + Enter in Ipynb&lt;/li&gt;
  &lt;li&gt;Try things with your own data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Infinitely flexible function, all purpose parameter fitting and tuning which is fast and scalable.&lt;/li&gt;
  &lt;li&gt;Neural network is the universal approximation function.&lt;/li&gt;
  &lt;li&gt;Gradient Descent &amp;amp; Backpropagation is the tuning part.&lt;/li&gt;
  &lt;li&gt;NVIDIA GPU supports CUDA which is efficient in running deep learning computations.&lt;/li&gt;
  &lt;li&gt;P2 and t2.micro&lt;/li&gt;
  &lt;li&gt;AWS Setup&lt;/li&gt;
  &lt;li&gt;AMI: Amazon Machine Images: Snapshot of instance at a given point in time&lt;/li&gt;
  &lt;li&gt;Literate programming&lt;/li&gt;
  &lt;li&gt;Cmd + Shift + P for keyboard shortcuts on jupyter notebook&lt;/li&gt;
  &lt;li&gt;tmux&lt;/li&gt;
  &lt;li&gt;Test, Train and Validation Datasets. Also, introduced to sample data.&lt;/li&gt;
  &lt;li&gt;Magic function&lt;/li&gt;
  &lt;li&gt;Pretrained Model - Model with Learned Parameters&lt;/li&gt;
  &lt;li&gt;VGG 16 model&lt;/li&gt;
  &lt;li&gt;Keras.json &amp;amp; theanorc file for configuration&lt;/li&gt;
  &lt;li&gt;Concept of Batch and Mini Batch&lt;/li&gt;
  &lt;li&gt;GPU has limited memory and the data transfer is a costly operation&lt;/li&gt;
  &lt;li&gt;Finetuning replaces the last layer of a pretrained network with the current classes&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 28 Dec 2017 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2017/12/notes-for-deep-learning-fast-ai/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/12/notes-for-deep-learning-fast-ai/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Design Patterns</title>
        <description>&lt;p&gt;Lesson 0:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Experience Reuse&lt;/li&gt;
  &lt;li&gt;Obeserver Pattern&lt;/li&gt;
  &lt;li&gt;Decorator Pattern&lt;/li&gt;
  &lt;li&gt;Factory Pattern&lt;/li&gt;
  &lt;li&gt;Singleton Pattern&lt;/li&gt;
  &lt;li&gt;Command Pattern&lt;/li&gt;
  &lt;li&gt;Adapter and Facade Pattern&lt;/li&gt;
  &lt;li&gt;Template Method Pattern&lt;/li&gt;
  &lt;li&gt;Iterator and Composite Pattern&lt;/li&gt;
  &lt;li&gt;State Pattern&lt;/li&gt;
  &lt;li&gt;Proxy Pattern&lt;/li&gt;
  &lt;li&gt;Compound Pattern&lt;/li&gt;
  &lt;li&gt;Design Pattern&lt;/li&gt;
  &lt;li&gt;Leftover Pattern&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 1:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Experience reuse&lt;/li&gt;
  &lt;li&gt;Change is the only constant&lt;/li&gt;
  &lt;li&gt;Exercise answers
    &lt;ul&gt;
      &lt;li&gt;Need to improve code for scalabitlity&lt;/li&gt;
      &lt;li&gt;Prototyping for experiments and then building products if the prototype is successful&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Design Principles
    &lt;ul&gt;
      &lt;li&gt;Identify aspects of app which vary and encapsulate them&lt;/li&gt;
      &lt;li&gt;Program to an interface not an implementation&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Behavious Class&lt;/li&gt;
  &lt;li&gt;Interface&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 18 Dec 2017 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2017/12/notes-for-head-first-design-patterns/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/12/notes-for-head-first-design-patterns/</guid>
        
        <category>Book</category>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for CNN for Visual Recognition!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Today is age of images/video but image/video data is hard to use&lt;/li&gt;
  &lt;li&gt;Computer Vision as an interdisciplinary field&lt;/li&gt;
  &lt;li&gt;Eye as an inspiration for big bang of species&lt;/li&gt;
  &lt;li&gt;Vision important for speciation in the early evolution of the species&lt;/li&gt;
  &lt;li&gt;Mechanical Vision i.e. Camera models - Camera Obscura&lt;/li&gt;
  &lt;li&gt;Hubel and Wiesel experiment&lt;/li&gt;
  &lt;li&gt;Primary visual cortex is very far away from eye&lt;/li&gt;
  &lt;li&gt;Simple structures excite neurons in human brain&lt;/li&gt;
  &lt;li&gt;Block World models - Visual world simplified into basic geometrical shapes&lt;/li&gt;
  &lt;li&gt;Book Vision by david marr&lt;/li&gt;
  &lt;li&gt;Hierarchical Representation&lt;/li&gt;
  &lt;li&gt;Generalized Cylinder and Pictorial Structure models&lt;/li&gt;
  &lt;li&gt;Normalized Cut - perceptual grouping problem - image segmentation&lt;/li&gt;
  &lt;li&gt;Real time Face Detection by Fuji Films Camera&lt;/li&gt;
  &lt;li&gt;Major focus is recognition&lt;/li&gt;
  &lt;li&gt;Engineered Features like SIFT, HOG&lt;/li&gt;
  &lt;li&gt;Spatial pyramid matching for scene recognition&lt;/li&gt;
  &lt;li&gt;Deformable Part Model&lt;/li&gt;
  &lt;li&gt;PASCAL visual object challenge&lt;/li&gt;
  &lt;li&gt;Imagenet&lt;/li&gt;
  &lt;li&gt;Image classification really useful for making progress in other image machine learning problems like image detection, segmentation, image captioning&lt;/li&gt;
  &lt;li&gt;Semantic Segmentation and Perceptual Grouping&lt;/li&gt;
  &lt;li&gt;Tell a story given a scene&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 2:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Image Classification&lt;/li&gt;
  &lt;li&gt;Semantic Gap - Representation of image on computer as numbers&lt;/li&gt;
  &lt;li&gt;Challenges are:
    &lt;ul&gt;
      &lt;li&gt;Viewpoint Variation - Camera rotations lead to changes in brightness.&lt;/li&gt;
      &lt;li&gt;Illumination issues&lt;/li&gt;
      &lt;li&gt;Deformation&lt;/li&gt;
      &lt;li&gt;Occlusion&lt;/li&gt;
      &lt;li&gt;Background Clutter&lt;/li&gt;
      &lt;li&gt;Intraclass Variation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Follow a data-driven approach i.e. Collect dataset of images and labels, use ML algos to train an image 
classifier and evaluate classifier on test images.&lt;/li&gt;
  &lt;li&gt;Nearest Neighbour Classifier - Use Manhattan distance&lt;/li&gt;
  &lt;li&gt;Do more compute at train time but prediction should be constant time computation.&lt;/li&gt;
  &lt;li&gt;Aside - Approximate Nearest Neighbour (FLANN)&lt;/li&gt;
  &lt;li&gt;Hyper-parameter - Distance, and the value of k for kNN&lt;/li&gt;
  &lt;li&gt;Training data, validation data and training data set or Cross Validation&lt;/li&gt;
  &lt;li&gt;Linear Classification&lt;/li&gt;
  &lt;li&gt;NN can see, hear, translate, control and think.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lecture 4:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Forward pass gives loss and backward pass gives gradients&lt;/li&gt;
  &lt;li&gt;Gradient Descent
    &lt;ul&gt;
      &lt;li&gt;Numerical which is slow and approx but easy to write&lt;/li&gt;
      &lt;li&gt;Analytical which is fast and exact but error prone&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Computational Graph is huge for Neural Turing Machine&lt;/li&gt;
  &lt;li&gt;Chain rule for backprop&lt;/li&gt;
  &lt;li&gt;Local gradients are computed at the time of forward pass and can be chained to global gradient later at the time of backprop.&lt;/li&gt;
  &lt;li&gt;For plus gate, during back prop, the value for the next gate is 1 * the previous value.&lt;/li&gt;
  &lt;li&gt;For multiplicative gate, during back prop, the value for the next gate is the value of other input * the previous value.&lt;/li&gt;
  &lt;li&gt;Hence, add gate is gradient distributor, the max gate is gradient router and mul gate can be the gradient switcher.&lt;/li&gt;
  &lt;li&gt;At branches, gradients are added according to multivariate chain rule.&lt;/li&gt;
  &lt;li&gt;Graph class with nodes topologically sorted and forward and backward function&lt;/li&gt;
  &lt;li&gt;Lot of memory required to store intermediate results that will be used during back prop&lt;/li&gt;
  &lt;li&gt;For vectors, we have jacobian matrix which stores derivative of each element of output wrt input&lt;/li&gt;
  &lt;li&gt;Vectorized Operations&lt;/li&gt;
  &lt;li&gt;Jacobian matrix is not always a full matrix and is a sparse matrix because there are values only on the diagonal and even not all those values are be used.&lt;/li&gt;
  &lt;li&gt;Backpropagation is recursive application of chain rule along computational graph to computer gradients  of all inputs/params/intermediates&lt;/li&gt;
  &lt;li&gt;Biological description of neurons
    &lt;ul&gt;
      &lt;li&gt;Soma: cell body&lt;/li&gt;
      &lt;li&gt;Dendrites: listeners/input&lt;/li&gt;
      &lt;li&gt;Axon: terminals/output&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Activation functions
    &lt;ul&gt;
      &lt;li&gt;Sigmoid&lt;/li&gt;
      &lt;li&gt;tanh&lt;/li&gt;
      &lt;li&gt;ReLU&lt;/li&gt;
      &lt;li&gt;Maxout&lt;/li&gt;
      &lt;li&gt;Leaky ReLU&lt;/li&gt;
      &lt;li&gt;ELU&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Fully connected layer and hidden layers&lt;/li&gt;
  &lt;li&gt;Kernel trick changes data representation to a space where itâ€™s linearly separable&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lecture 7:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CNN operate over volumes.&lt;/li&gt;
  &lt;li&gt;Filters are convolved over the image ie the filter is slid over the image spatially computing dot products which result in activation map.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\text{output_size} = ((N-F+2*P)/S) + 1&lt;/script&gt; where N is width/height, F is filter size, P is padding and S is stride.&lt;/li&gt;
  &lt;li&gt;Input padding is a common practice since we want to preserve sizes spatially otherwise the size of the input decreases sharply.&lt;/li&gt;
  &lt;li&gt;To always achieve same output volume spatially for stride of 1, use &lt;script type=&quot;math/tex&quot;&gt;(F-1)/2&lt;/script&gt; zero padding&lt;/li&gt;
  &lt;li&gt;K, N, F and P are hyperparameters where K is the number of filters and is in powers of 2 as certains subroutines are efficient in computations with a power of 2.&lt;/li&gt;
  &lt;li&gt;The depth of the output of a convolution will the total number of filters.&lt;/li&gt;
  &lt;li&gt;With parameter sharing it introduces, &lt;script type=&quot;math/tex&quot;&gt;F*F*D_1&lt;/script&gt; weights per filter.&lt;/li&gt;
  &lt;li&gt;1*1 convolutions are important since they return the same sized output since we do dot products over the full depth of the volume (depth columns or fibres).&lt;/li&gt;
  &lt;li&gt;The size of F is usually odd.&lt;/li&gt;
  &lt;li&gt;Usually, images are preprocessed to squares.&lt;/li&gt;
  &lt;li&gt;The filter is also called kernel. Filters capture local information.&lt;/li&gt;
  &lt;li&gt;Along the depth of the output volume, all the neurons have actually looked at the same patch but their weights will still be different.&lt;/li&gt;
  &lt;li&gt;Pooling layer makes the representations smaller and managable&lt;/li&gt;
  &lt;li&gt;Pooling operates over each activation map independently&lt;/li&gt;
  &lt;li&gt;LeNet - 5&lt;/li&gt;
  &lt;li&gt;AlexnNet
    &lt;ul&gt;
      &lt;li&gt;Use of ReLU&lt;/li&gt;
      &lt;li&gt;Used norm layers&lt;/li&gt;
      &lt;li&gt;Heavy data augmentation&lt;/li&gt;
      &lt;li&gt;Dropout&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ZFNet&lt;/li&gt;
  &lt;li&gt;VGGNet&lt;/li&gt;
  &lt;li&gt;GoogleNet&lt;/li&gt;
  &lt;li&gt;ResNet
    &lt;ul&gt;
      &lt;li&gt;Skip Step&lt;/li&gt;
      &lt;li&gt;Batch normalization layer and hence can use a higher learning rate&lt;/li&gt;
      &lt;li&gt;No dropout&lt;/li&gt;
      &lt;li&gt;Xavier/2 initialization&lt;/li&gt;
      &lt;li&gt;Faster than VGGNet(20 layers) inspite of having 152 layers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Policy network&lt;/li&gt;
  &lt;li&gt;As we go ahead with the architectures, we find that the numbers of parameters are reduced but more conputation is required and the results are too promising.&lt;/li&gt;
  &lt;li&gt;Instead of fully connected layers, use average pooling layers in the end of a CNN.&lt;/li&gt;
  &lt;li&gt;Convnets stack - CONV, POOL, Fully Connected layers&lt;/li&gt;
  &lt;li&gt;Trend towards smaller filters and deeper networks and getting rid of POOL/FC layers&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 09 Oct 2017 15:30:12 +0530</pubDate>
        <link>http://localhost:4000/2017/10/notes-for-CNN-for-visual-recognition/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/10/notes-for-CNN-for-visual-recognition/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Deep Learning STAT 946</title>
        <description>&lt;p&gt;Lesson 6:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Network with atleast one convolutional layer&lt;/li&gt;
  &lt;li&gt;Three stages
    &lt;ul&gt;
      &lt;li&gt;Convolution function (Discrete Convolution)&lt;/li&gt;
      &lt;li&gt;Detection Eg. ReLU&lt;/li&gt;
      &lt;li&gt;Pooling&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Convolution vs Cross Corelation (Replace + by - in convolution)&lt;/li&gt;
  &lt;li&gt;Convolved Feature or Feature Map - captures features in image similar to kernel&lt;/li&gt;
  &lt;li&gt;Sharing parameters&lt;/li&gt;
  &lt;li&gt;Making the network sparse&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 22 Jun 2017 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2017/06/notes-for-deep-learning-waterloo/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/06/notes-for-deep-learning-waterloo/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Artificial Neural Networks!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;He defines the terminologies as weights denoted by w, bias denoted by b, activation function denoted by g(.), neuron preactivation(I/P) denoted by x, neuron activation(O/P) denoted by h(x). Everything exept b is a vector.&lt;/li&gt;
  &lt;li&gt;He starts a discussion about potential activation functions in ANN.
    &lt;ul&gt;
      &lt;li&gt;Linear Activation Function, g(a) = a, No squashing of I/P
 	- Sigmoid Function, g(a) = 1/(1+e^(-a)), Lies between 0 and 1, always +ve, bounded, strictly increasing.
 	- Tanh Function, g(a) = (e^(2a)-1)/(e^(2a)+1), Lies netween -1 and 1, bounded, strictly increasing
 	- ReLU Function, g(a) = max(0, a), Bounded below 0, Strictly increasing, tends to give neurons with sparse activities because it is 0 over a large range of -ve numbers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;He further discusses the complexity of computations the neural network can perform.&lt;/li&gt;
  &lt;li&gt;He talks about linearly separable problems(binary classification problems) which have a decision boundary and use single neuron to perform logistic regression. This can be achieved using sigmoid.&lt;/li&gt;
  &lt;li&gt;He goes further and talks about alternate representations of input vector which can convert the non lineraly separable problem into a separable one. For eg. XOR can be represented by AND(X^, Y) on x axis and AND(X, Y^) on y axis. He tries to break the problem into pieces and check if single piece of problem can be represented by a neuron or a group of neurons and then combine all the representations to generate a representation for non linearly separable problem.&lt;/li&gt;
  &lt;li&gt;He discusses the hidden layer pre-activation, hidden layer activation and output layer activation for a single layer neural network. Going ahead with multilayer neural network, the only difference is that multilayer neural network has multiple hidden layers.
    &lt;ul&gt;
      &lt;li&gt;To perform binary classification, one can use sigmoid as output activation function.&lt;/li&gt;
      &lt;li&gt;For multiclass classification one needs multiple outputs, so one uses conditional probablity 
  p(y = c|x) where c is the class and then using softmax activation function; A function which sums to 1 when all the probablities are added. Additionally, softmax is strictly positive.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;He discusses universal approximation theorm i.e. a single layer hidden neural network with linear output unit which can approx. any continuous function arbitrarily well, given enough hidden units.&lt;/li&gt;
  &lt;li&gt;He discusses the inspiration that neural networks draw from biological neurons and talks about the idea of different layers and how they work in parallel with the visual cortex. He uses edges and points to introduce the idea of complex shapes formed using these simple structures and uses face detection as an example. He defines the following terms too.
    &lt;ul&gt;
      &lt;li&gt;Action potential is an electrical impulse that travels through the axon.&lt;/li&gt;
      &lt;li&gt;Firing rate of a neuron is frequency with which a neuron can spike.&lt;/li&gt;
      &lt;li&gt;Neurons tend to excite or inhibit other neurons.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Analogy between artificial NN and biological NN
    &lt;ul&gt;
      &lt;li&gt;Activation function is analogous to firing rate.&lt;/li&gt;
      &lt;li&gt;Weights in ANN decide the excitation or inhibition of a neuron.&lt;/li&gt;
      &lt;li&gt;Activation function and bias model the action potential of a neuron.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 2:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;He starts with discussion on empirical risk minimization also called structural risk minimization 
 when using regularization.&lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\arg\max_{\theta} \frac{1}{T} \sum_{t} l(f(x^t; \theta), y^t) + \lambda \Omega (\theta)&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\Omega (\theta)&lt;/script&gt; is a regularizer&lt;/li&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;l(f(x^t; \theta), y^t)&lt;/script&gt; is a loss function&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Learning hence becomes an optimization problem&lt;/li&gt;
  &lt;li&gt;Ideally should optimize classification error but itâ€™s not smooth as itâ€™s either 0 or 1 so the above loss function is a surrogate for what we should truly optimise&lt;/li&gt;
  &lt;li&gt;Stochastic Gradient Descent
    &lt;ul&gt;
      &lt;li&gt;Initialization of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; i.e. weights and biases.&lt;/li&gt;
      &lt;li&gt;for N interations
        &lt;ul&gt;
          &lt;li&gt;for each training example
            &lt;ul&gt;
              &lt;li&gt;figure out a direction for updating the parameters. Take the negative gradient of loss and regularizer&lt;/li&gt;
              &lt;li&gt;Use the above gradient value along with &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; ie learning rate to update the parameters&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Training epoch is equal to iteration over all the examples&lt;/li&gt;
  &lt;li&gt;Neural network estimates &lt;script type=&quot;math/tex&quot;&gt;f(x)_c = p(y = c \lvert x)&lt;/script&gt; for classification task&lt;/li&gt;
  &lt;li&gt;Aim is to Maximize probability of &lt;script type=&quot;math/tex&quot;&gt;y^{(t)} \ given \ x^{(t)}&lt;/script&gt; so we minimize - log likelihood 
 which is &lt;script type=&quot;math/tex&quot;&gt;l(f(x), y) = - log \ f(x)_{y}&lt;/script&gt; also called as cross entropy&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 18 Apr 2017 17:15:34 +0530</pubDate>
        <link>http://localhost:4000/2017/04/notes-for-youtube-course-neural-networks/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/04/notes-for-youtube-course-neural-networks/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Intro to Data Analysis!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data Analysis Process
    &lt;ul&gt;
      &lt;li&gt;Question&lt;/li&gt;
      &lt;li&gt;Wrangle
        &lt;ul&gt;
          &lt;li&gt;Acquiring Data&lt;/li&gt;
          &lt;li&gt;Cleaning Data&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Explore&lt;/li&gt;
      &lt;li&gt;Draw conclusions&lt;/li&gt;
      &lt;li&gt;Communicate&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The process is iterative and need to go back and forth.&lt;/li&gt;
  &lt;li&gt;Each row of csv can be represented in two ways in python
    &lt;ul&gt;
      &lt;li&gt;As a list&lt;/li&gt;
      &lt;li&gt;As a dictionary&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 24 Mar 2017 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2017/03/notes-for-udacity-intro-to-data-analysis/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/notes-for-udacity-intro-to-data-analysis/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for ML!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ML is algorithms for inferring unknowns from knowns. eg. Filtering out spam, Detect Handwriting, Face Detection, Speech Recognition, Netflix ranking, Navigation, Climate Modelling&lt;/li&gt;
  &lt;li&gt;Classes of ML
    &lt;ul&gt;
      &lt;li&gt;Supervised vs unsupervised Learning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Supervised Learning is given data &lt;script type=&quot;math/tex&quot;&gt;(x_1, y_1), (x_2, y_2), (x_3, y_3), .............  (x_n, y_n)&lt;/script&gt;, choose a function f(x) = y where &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; = data point and &lt;script type=&quot;math/tex&quot;&gt;y_i = class/value&lt;/script&gt; and then generalise for new values of x. i.e f(x) &lt;script type=&quot;math/tex&quot;&gt;\to&lt;/script&gt; y&lt;/li&gt;
  &lt;li&gt;Two types of Supervised Learning problems
    &lt;ul&gt;
      &lt;li&gt;Classification - &lt;script type=&quot;math/tex&quot;&gt;y_i \in \{\text{finite set}\}&lt;/script&gt;.&lt;/li&gt;
      &lt;li&gt;Regression -  &lt;script type=&quot;math/tex&quot;&gt;y_i \in \{R\}&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;y_i \in \{R^d\}&lt;/script&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unsupervised Learning is given data &lt;script type=&quot;math/tex&quot;&gt;(x_1, x_2, x_3,........x_n)&lt;/script&gt; i.e. &lt;script type=&quot;math/tex&quot;&gt;x_i \in \{R^d\}&lt;/script&gt;
 Find patterns in data
    &lt;ul&gt;
      &lt;li&gt;Clustering&lt;/li&gt;
      &lt;li&gt;Density Estimation&lt;/li&gt;
      &lt;li&gt;Dimensionality Reduction should project it down preserving the structure of data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Variations on supervised and unsupervised learning
    &lt;ul&gt;
      &lt;li&gt;Semi Supervised Learning - (x1, y1), (x2, y2), (x3, y3), â€¦â€¦â€¦â€¦.  (xk, yk), xk+1, xk+2 â€¦.xn,
  predict yk+1, yk+2 â€¦.. yn. eg.&lt;/li&gt;
      &lt;li&gt;Active Learning&lt;/li&gt;
      &lt;li&gt;Decision Theory&lt;/li&gt;
      &lt;li&gt;Reinforcement Learning - maximize overall reward and minimize overall losses.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generative vs Discriminative Models
    &lt;ul&gt;
      &lt;li&gt;Discriminative = P(y given x) which is Conditional Probability&lt;/li&gt;
      &lt;li&gt;Generative = P(x and y) = f(x given y) p(y) = p(y given x) f(x) - models joint distribution - more powerful than Discriminative since using more parameters&lt;/li&gt;
      &lt;li&gt;Estimating a density is difficult and need a lot of data leading to high variance and hence Generative model will have bad performance than Discriminative.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;kNN
    &lt;ul&gt;
      &lt;li&gt;circle concept that is used to decide the class of the test point&lt;/li&gt;
      &lt;li&gt;Probabilistic interpretation - (y)&lt;/li&gt;
      &lt;li&gt;Discriminative model&lt;/li&gt;
      &lt;li&gt;Bias - variance tradeoff&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 19 Mar 2017 17:15:34 +0530</pubDate>
        <link>http://localhost:4000/2017/03/notes-for-youtube-ml-mathematicalmonk/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/notes-for-youtube-ml-mathematicalmonk/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Segmentation and Clustering!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Process of Segmentation&lt;/li&gt;
  &lt;li&gt;Standardisation where we treat a group as a whole&lt;/li&gt;
  &lt;li&gt;Localisation where we treat them as individuals&lt;/li&gt;
  &lt;li&gt;Grouping or binning&lt;/li&gt;
  &lt;li&gt;It becomes difficult to group as the number of variables increase&lt;/li&gt;
  &lt;li&gt;Clustering is a mathematical procedure for multidimensional analysis&lt;/li&gt;
  &lt;li&gt;For a given set of objects, this procedure groups similar objects into clusters&lt;/li&gt;
  &lt;li&gt;Distance is way to measure similarity&lt;/li&gt;
  &lt;li&gt;Intra-cluster separation is minimised and inter-cluster distance is maximised&lt;/li&gt;
  &lt;li&gt;eg. Fraudulent insurance Claim, User Segmentation, Bioinformatics Study, Education&lt;/li&gt;
  &lt;li&gt;Supervised and Unsupervised Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 2:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Selecting data based on objectives&lt;/li&gt;
  &lt;li&gt;Use demographic and social data in first phases and remove historical data.&lt;/li&gt;
  &lt;li&gt;Predetermined Bias in Transactional Data - A/B testing&lt;/li&gt;
  &lt;li&gt;Data types in clustering
    &lt;ul&gt;
      &lt;li&gt;Continuous or Ordinal Variables&lt;/li&gt;
      &lt;li&gt;Outliers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Scaling data because distance is important in cluster analysis
    &lt;ul&gt;
      &lt;li&gt;Z score or standard score i.e. no. of SD away from the mean&lt;/li&gt;
      &lt;li&gt;Unit interval compressing all values between 0 and 1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transforming Variables and visualising the data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 3:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Variable Reduction Procedures help in accounting for most of the variables 
 in all of the observed variables&lt;/li&gt;
  &lt;li&gt;Principle variables or artificial variables explain most of the variance&lt;/li&gt;
  &lt;li&gt;Try to use variable reduction when variables are somewhat related&lt;/li&gt;
  &lt;li&gt;Need to weight benefits one can get from reducing the no. of variables used in 
 clustering analysis, with the all important aspect of needing to be able to explain
 the basis for clusters&lt;/li&gt;
  &lt;li&gt;Types of Variable Reduction Procedures
    &lt;ul&gt;
      &lt;li&gt;Factor Analysis - Correlation between variables, trying to figure out causes&lt;/li&gt;
      &lt;li&gt;Principal Component Analysis - accounts for total variation, trying to summarise&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Factor Analysis makes assumptions for underlying causal model while PCA doesnâ€™t&lt;/li&gt;
  &lt;li&gt;In PCA, combine variables that are related to a common story&lt;/li&gt;
  &lt;li&gt;Loadings and Values
    &lt;ul&gt;
      &lt;li&gt;Close to zero loading has less value on components&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 19 Jan 2017 17:15:34 +0530</pubDate>
        <link>http://localhost:4000/2017/01/notes-for-segmentation-and-clustering/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/01/notes-for-segmentation-and-clustering/</guid>
        
        <category>Notes</category>
        
        
      </item>
    
  </channel>
</rss>
