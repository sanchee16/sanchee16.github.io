<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sancheeta Kaushal</title>
    <description>Strong convictions precede great actions.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 08 Dec 2017 16:37:51 +0530</pubDate>
    <lastBuildDate>Fri, 08 Dec 2017 16:37:51 +0530</lastBuildDate>
    <generator>Jekyll v3.5.2</generator>
    
      <item>
        <title>Notes for CNN for Visual Recognition!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Today is age of images/video but image/video data is hard to use&lt;/li&gt;
  &lt;li&gt;Computer Vision as an interdisciplinary field&lt;/li&gt;
  &lt;li&gt;Eye as an inspiration for big bang of species&lt;/li&gt;
  &lt;li&gt;Vision important for speciation in the early evolution of the species&lt;/li&gt;
  &lt;li&gt;Mechanical Vision i.e. Camera models - Camera Obscura&lt;/li&gt;
  &lt;li&gt;Hubel and Wiesel experiment&lt;/li&gt;
  &lt;li&gt;Primary visual cortex is very far away from eye&lt;/li&gt;
  &lt;li&gt;Simple structures excite neurons in human brain&lt;/li&gt;
  &lt;li&gt;Block World models - Visual world simplified into basic geometrical shapes&lt;/li&gt;
  &lt;li&gt;Book Vision by david marr&lt;/li&gt;
  &lt;li&gt;Hierarchical Representation&lt;/li&gt;
  &lt;li&gt;Generalized Cylinder and Pictorial Structure models&lt;/li&gt;
  &lt;li&gt;Normalized Cut - perceptual grouping problem - image segmentation&lt;/li&gt;
  &lt;li&gt;Real time Face Detection by Fuji Films Camera&lt;/li&gt;
  &lt;li&gt;Major focus is recognition&lt;/li&gt;
  &lt;li&gt;Engineered Features like SIFT, HOG&lt;/li&gt;
  &lt;li&gt;Spatial pyramid matching for scene recognition&lt;/li&gt;
  &lt;li&gt;Deformable Part Model&lt;/li&gt;
  &lt;li&gt;PASCAL visual object challenge&lt;/li&gt;
  &lt;li&gt;Imagenet&lt;/li&gt;
  &lt;li&gt;Image classification really useful for making progress in other image machine learning problems like image detection, segmentation, image captioning&lt;/li&gt;
  &lt;li&gt;Semantic Segmentation and Perceptual Grouping&lt;/li&gt;
  &lt;li&gt;Tell a story given a scene&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 2:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Image Classification&lt;/li&gt;
  &lt;li&gt;Semantic Gap - Representation of image on computer as numbers&lt;/li&gt;
  &lt;li&gt;Challenges are:
    &lt;ul&gt;
      &lt;li&gt;Viewpoint Variation - Camera rotations lead to changes in brightness.&lt;/li&gt;
      &lt;li&gt;Illumination issues&lt;/li&gt;
      &lt;li&gt;Deformation&lt;/li&gt;
      &lt;li&gt;Occlusion&lt;/li&gt;
      &lt;li&gt;Background Clutter&lt;/li&gt;
      &lt;li&gt;Intraclass Variation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Follow a data-driven approach i.e. Collect dataset of images and labels, use ML algos to train an image 
classifier and evaluate classifier on test images.&lt;/li&gt;
  &lt;li&gt;Nearest Neighbour Classifier - Use Manhattan distance&lt;/li&gt;
  &lt;li&gt;Do more compute at train time but prediction should be constant time computation.&lt;/li&gt;
  &lt;li&gt;Aside - Approximate Nearest Neighbour (FLANN)&lt;/li&gt;
  &lt;li&gt;Hyper-parameter - Distance, and the value of k for kNN&lt;/li&gt;
  &lt;li&gt;Training data, validation data and training data set or Cross Validation&lt;/li&gt;
  &lt;li&gt;Linear Classification&lt;/li&gt;
  &lt;li&gt;NN can see, hear, translate, control and think.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lecture 4:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Forward pass gives loss and backward pass gives gradients&lt;/li&gt;
  &lt;li&gt;Gradient Descent
    &lt;ul&gt;
      &lt;li&gt;Numerical which is slow and approx but easy to write&lt;/li&gt;
      &lt;li&gt;Analytical which is fast and exact but error prone&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Computational Graph is huge for Neural Turing Machine&lt;/li&gt;
  &lt;li&gt;Chain rule for backprop&lt;/li&gt;
  &lt;li&gt;Local gradients are computed at the time of forward pass and can be chained to global gradient later at the time of backprop.&lt;/li&gt;
  &lt;li&gt;For plus gate, during back prop, the value for the next gate is 1 * the previous value.&lt;/li&gt;
  &lt;li&gt;For multiplicative gate, during back prop, the value for the next gate is the value of other input * the previous value.&lt;/li&gt;
  &lt;li&gt;Hence, add gate is gradient distributor, the max gate is gradient router and mul gate can be the gradient switcher.&lt;/li&gt;
  &lt;li&gt;At branches, gradients are added according to multivariate chain rule.&lt;/li&gt;
  &lt;li&gt;Graph class with nodes topologically sorted and forward and backward function&lt;/li&gt;
  &lt;li&gt;Lot of memory required to store intermediate results that will be used during back prop&lt;/li&gt;
  &lt;li&gt;For vectors, we have jacobian matrix which stores derivative of each element of output wrt input&lt;/li&gt;
  &lt;li&gt;Vectorized Operations&lt;/li&gt;
  &lt;li&gt;Jacobian matrix is not always a full matrix and is a sparse matrix because there are values only on the diagonal and even not all those values are be used.&lt;/li&gt;
  &lt;li&gt;Backpropagation is recursive application of chain rule along computational graph to computer gradients  of all inputs/params/intermediates&lt;/li&gt;
  &lt;li&gt;Biological description of neurons
    &lt;ul&gt;
      &lt;li&gt;Soma: cell body&lt;/li&gt;
      &lt;li&gt;Dendrites: listeners/input&lt;/li&gt;
      &lt;li&gt;Axon: terminals/output&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Activation functions
    &lt;ul&gt;
      &lt;li&gt;Sigmoid&lt;/li&gt;
      &lt;li&gt;tanh&lt;/li&gt;
      &lt;li&gt;ReLU&lt;/li&gt;
      &lt;li&gt;Maxout&lt;/li&gt;
      &lt;li&gt;Leaky ReLU&lt;/li&gt;
      &lt;li&gt;ELU&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Fully connected layer and hidden layers&lt;/li&gt;
  &lt;li&gt;Kernel trick changes data representation to a space where it’s linearly separable&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lecture 7:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CNN operate over volumes.&lt;/li&gt;
  &lt;li&gt;Filters are convolved over the image ie the filter is slid over the image spatially computing dot products which result in activation map.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\text{output_size} = ((N-F+2*P)/S) + 1&lt;/script&gt; where N is width/height, F is filter size, P is padding and S is stride.&lt;/li&gt;
  &lt;li&gt;Input padding is a common practice since we want to preserve sizes spatially otherwise the size of the input decreases sharply.&lt;/li&gt;
  &lt;li&gt;To always achieve same output volume spatially for stride of 1, use &lt;script type=&quot;math/tex&quot;&gt;(F-1)/2&lt;/script&gt; zero padding&lt;/li&gt;
  &lt;li&gt;K, N, F and P are hyperparameters where K is the number of filters and is in powers of 2 as certains subroutines are efficient in computations with a power of 2.&lt;/li&gt;
  &lt;li&gt;The depth of the output of a convolution will the total number of filters.&lt;/li&gt;
  &lt;li&gt;With parameter sharing it introduces, &lt;script type=&quot;math/tex&quot;&gt;F*F*D_1&lt;/script&gt; weights per filter.&lt;/li&gt;
  &lt;li&gt;1*1 convolutions are important since they return the same sized output since we do dot products over the full depth of the volume (depth columns or fibres).&lt;/li&gt;
  &lt;li&gt;The size of F is usually odd.&lt;/li&gt;
  &lt;li&gt;Usually, images are preprocessed to squares.&lt;/li&gt;
  &lt;li&gt;The filter is also called kernel. Filters capture local information.&lt;/li&gt;
  &lt;li&gt;Along the depth of the output volume, all the neurons have actually looked at the same patch but their weights will still be different.&lt;/li&gt;
  &lt;li&gt;Pooling layer makes the representations smaller and managable&lt;/li&gt;
  &lt;li&gt;Pooling operates over each activation map independently&lt;/li&gt;
  &lt;li&gt;LeNet - 5&lt;/li&gt;
  &lt;li&gt;AlexnNet
    &lt;ul&gt;
      &lt;li&gt;Use of ReLU&lt;/li&gt;
      &lt;li&gt;Used norm layers&lt;/li&gt;
      &lt;li&gt;Heavy data augmentation&lt;/li&gt;
      &lt;li&gt;Dropout&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ZFNet&lt;/li&gt;
  &lt;li&gt;VGGNet&lt;/li&gt;
  &lt;li&gt;GoogleNet&lt;/li&gt;
  &lt;li&gt;ResNet
    &lt;ul&gt;
      &lt;li&gt;Skip Step&lt;/li&gt;
      &lt;li&gt;Batch normalization layer and hence can use a higher learning rate&lt;/li&gt;
      &lt;li&gt;No dropout&lt;/li&gt;
      &lt;li&gt;Xavier/2 initialization&lt;/li&gt;
      &lt;li&gt;Faster than VGGNet(20 layers) inspite of having 152 layers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Policy network&lt;/li&gt;
  &lt;li&gt;As we go ahead with the architectures, we find that the numbers of parameters are reduced but more conputation is required and the results are too promising.&lt;/li&gt;
  &lt;li&gt;Instead of fully connected layers, use average pooling layers in the end of a CNN.&lt;/li&gt;
  &lt;li&gt;Convnets stack - CONV, POOL, Fully Connected layers&lt;/li&gt;
  &lt;li&gt;Trend towards smaller filters and deeper networks and getting rid of POOL/FC layers&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 09 Oct 2017 15:30:12 +0530</pubDate>
        <link>http://localhost:4000/2017/10/notes-for-CNN-for-visual-recognition/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/10/notes-for-CNN-for-visual-recognition/</guid>
        
        <category>notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Deep Learning STAT 946</title>
        <description>Lesson 6:

- Network with atleast one convolutional layer 
- Three stages 
	- Convolution function (Discrete Convolution)  
	- Detection Eg. ReLU
	- Pooling
- Convolution vs Cross Corelation (Replace + by - in convolution)
- Convolved Feature or Feature Map - captures features in image similar to kernel 
- Sharing parameters
- Making the network sparse
- </description>
        <pubDate>Thu, 22 Jun 2017 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2017/06/notes-for-deep-learning-waterloo/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/06/notes-for-deep-learning-waterloo/</guid>
        
        <category>notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Artificial Neural Networks!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;He defines the terminologies as weights denoted by w, bias denoted by b, activation function denoted by g(.), neuron preactivation(I/P) denoted by x, neuron activation(O/P) denoted by h(x). Everything exept b is a vector.&lt;/li&gt;
  &lt;li&gt;He starts a discussion about potential activation functions in ANN.
    &lt;ul&gt;
      &lt;li&gt;Linear Activation Function, g(a) = a, No squashing of I/P
 	- Sigmoid Function, g(a) = 1/(1+e^(-a)), Lies between 0 and 1, always +ve, bounded, strictly increasing.
 	- Tanh Function, g(a) = (e^(2a)-1)/(e^(2a)+1), Lies netween -1 and 1, bounded, strictly increasing
 	- ReLU Function, g(a) = max(0, a), Bounded below 0, Strictly increasing, tends to give neurons with sparse activities because it is 0 over a large range of -ve numbers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;He further discusses the complexity of computations the neural network can perform.&lt;/li&gt;
  &lt;li&gt;He talks about linearly separable problems(binary classification problems) which have a decision boundary and use single neuron to perform logistic regression. This can be achieved using sigmoid.&lt;/li&gt;
  &lt;li&gt;He goes further and talks about alternate representations of input vector which can convert the non lineraly separable problem into a separable one. For eg. XOR can be represented by AND(X^, Y) on x axis and AND(X, Y^) on y axis. He tries to break the problem into pieces and check if single piece of problem can be represented by a neuron or a group of neurons and then combine all the representations to generate a representation for non linearly separable problem.&lt;/li&gt;
  &lt;li&gt;He discusses the hidden layer pre-activation, hidden layer activation and output layer activation for a single layer neural network. Going ahead with multilayer neural network, the only difference is that multilayer neural network has multiple hidden layers.
    &lt;ul&gt;
      &lt;li&gt;To perform binary classification, one can use sigmoid as output activation function.&lt;/li&gt;
      &lt;li&gt;For multiclass classification one needs multiple outputs, so one uses conditional probablity 
  p(y = c|x) where c is the class and then using softmax activation function; A function which sums to 1 when all the probablities are added. Additionally, softmax is strictly positive.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;He discusses universal approximation theorm i.e. a single layer hidden neural network with linear output unit which can approx. any continuous function arbitrarily well, given enough hidden units.&lt;/li&gt;
  &lt;li&gt;He discusses the inspiration that neural networks draw from biological neurons and talks about the idea of different layers and how they work in parallel with the visual cortex. He uses edges and points to introduce the idea of complex shapes formed using these simple structures and uses face detection as an example. He defines the following terms too.
    &lt;ul&gt;
      &lt;li&gt;Action potential is an electrical impulse that travels through the axon.&lt;/li&gt;
      &lt;li&gt;Firing rate of a neuron is frequency with which a neuron can spike.&lt;/li&gt;
      &lt;li&gt;Neurons tend to excite or inhibit other neurons.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Analogy between artificial NN and biological NN
    &lt;ul&gt;
      &lt;li&gt;Activation function is analogous to firing rate.&lt;/li&gt;
      &lt;li&gt;Weights in ANN decide the excitation or inhibition of a neuron.&lt;/li&gt;
      &lt;li&gt;Activation function and bias model the action potential of a neuron.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 2:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;He starts with discussion on empirical risk minimization also called structural risk minimization 
 when using regularization.&lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\arg\max_{\theta} \frac{1}{T} \sum_{t} l(f(x^t; \theta), y^t) + \lambda \Omega (\theta)&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\Omega (\theta)&lt;/script&gt; is a regularizer&lt;/li&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;l(f(x^t; \theta), y^t)&lt;/script&gt; is a loss function&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Learning hence becomes an optimization problem&lt;/li&gt;
  &lt;li&gt;Ideally should optimize classification error but it’s not smooth as it’s either 0 or 1 so the above loss function is a surrogate for what we should truly optimise&lt;/li&gt;
  &lt;li&gt;Stochastic Gradient Descent
    &lt;ul&gt;
      &lt;li&gt;Initialization of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; i.e. weights and biases.&lt;/li&gt;
      &lt;li&gt;for N interations
        &lt;ul&gt;
          &lt;li&gt;for each training example
            &lt;ul&gt;
              &lt;li&gt;figure out a direction for updating the parameters. Take the negative gradient of loss and regularizer&lt;/li&gt;
              &lt;li&gt;Use the above gradient value along with &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; ie learning rate to update the parameters&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Training epoch is equal to iteration over all the examples&lt;/li&gt;
  &lt;li&gt;Neural network estimates &lt;script type=&quot;math/tex&quot;&gt;f(x)_c = p(y = c \lvert x)&lt;/script&gt; for classification task&lt;/li&gt;
  &lt;li&gt;Aim is to Maximize probability of &lt;script type=&quot;math/tex&quot;&gt;y^{(t)} \ given \ x^{(t)}&lt;/script&gt; so we minimize - log likelihood 
 which is &lt;script type=&quot;math/tex&quot;&gt;l(f(x), y) = - log \ f(x)_{y}&lt;/script&gt; also called as cross entropy&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 18 Apr 2017 17:15:34 +0530</pubDate>
        <link>http://localhost:4000/2017/04/notes-for-youtube-course-neural-networks/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/04/notes-for-youtube-course-neural-networks/</guid>
        
        <category>notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Intro to Data Analysis!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data Analysis Process
    &lt;ul&gt;
      &lt;li&gt;Question&lt;/li&gt;
      &lt;li&gt;Wrangle
        &lt;ul&gt;
          &lt;li&gt;Acquiring Data&lt;/li&gt;
          &lt;li&gt;Cleaning Data&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Explore&lt;/li&gt;
      &lt;li&gt;Draw conclusions&lt;/li&gt;
      &lt;li&gt;Communicate&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The process is iterative and need to go back and forth.&lt;/li&gt;
  &lt;li&gt;Each row of csv can be represented in two ways in python
    &lt;ul&gt;
      &lt;li&gt;As a list&lt;/li&gt;
      &lt;li&gt;As a dictionary&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 24 Mar 2017 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2017/03/notes-for-udacity-intro-to-data-analysis/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/notes-for-udacity-intro-to-data-analysis/</guid>
        
        <category>notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Deep Learning fast.ai MOOC!</title>
        <description>&lt;p&gt;Lesson 0:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Incredibly flexible function (NN), all-purpose parameter fitting(Backprop), fast and scalable(GPU) made DL possible&lt;/li&gt;
  &lt;li&gt;?? before any function describes the function in Ipynb&lt;/li&gt;
  &lt;li&gt;Shift + Enter in Ipynb&lt;/li&gt;
  &lt;li&gt;Try things with your own data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Infinitely flexible function, all purpose parameter tuning which is fast and scalable.&lt;/li&gt;
  &lt;li&gt;NVIDIA GPU supports CUDA which is efficient in running deep learning computations.&lt;/li&gt;
  &lt;li&gt;P2 and t2.micro&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 24 Mar 2017 06:53:34 +0530</pubDate>
        <link>http://localhost:4000/2017/03/notes-for-deep-learning-fast-ai/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/notes-for-deep-learning-fast-ai/</guid>
        
        <category>notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for ML!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ML is algorithms for inferring unknowns from knowns. eg. Filtering out spam, Detect Handwriting, Face Detection, Speech Recognition, Netflix ranking, Navigation, Climate Modelling&lt;/li&gt;
  &lt;li&gt;Classes of ML
    &lt;ul&gt;
      &lt;li&gt;Supervised vs unsupervised Learning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Supervised Learning is given data &lt;script type=&quot;math/tex&quot;&gt;(x_1, y_1), (x_2, y_2), (x_3, y_3), .............  (x_n, y_n)&lt;/script&gt;, choose a function f(x) = y where &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; = data point and &lt;script type=&quot;math/tex&quot;&gt;y_i = class/value&lt;/script&gt; and then generalise for new values of x. i.e f(x) &lt;script type=&quot;math/tex&quot;&gt;\to&lt;/script&gt; y&lt;/li&gt;
  &lt;li&gt;Two types of Supervised Learning problems
    &lt;ul&gt;
      &lt;li&gt;Classification - &lt;script type=&quot;math/tex&quot;&gt;y_i \in \{\text{finite set}\}&lt;/script&gt;.&lt;/li&gt;
      &lt;li&gt;Regression -  &lt;script type=&quot;math/tex&quot;&gt;y_i \in \{R\}&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;y_i \in \{R^d\}&lt;/script&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unsupervised Learning is given data &lt;script type=&quot;math/tex&quot;&gt;(x_1, x_2, x_3,........x_n)&lt;/script&gt; i.e. &lt;script type=&quot;math/tex&quot;&gt;x_i \in \{R^d\}&lt;/script&gt;
 Find patterns in data
    &lt;ul&gt;
      &lt;li&gt;Clustering&lt;/li&gt;
      &lt;li&gt;Density Estimation&lt;/li&gt;
      &lt;li&gt;Dimensionality Reduction should project it down preserving the structure of data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Variations on supervised and unsupervised learning
    &lt;ul&gt;
      &lt;li&gt;Semi Supervised Learning - (x1, y1), (x2, y2), (x3, y3), ………….  (xk, yk), xk+1, xk+2 ….xn,
  predict yk+1, yk+2 ….. yn. eg.&lt;/li&gt;
      &lt;li&gt;Active Learning&lt;/li&gt;
      &lt;li&gt;Decision Theory&lt;/li&gt;
      &lt;li&gt;Reinforcement Learning - maximize overall reward and minimize overall losses.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generative vs Discriminative Models
    &lt;ul&gt;
      &lt;li&gt;Discriminative = P(y given x) which is Conditional Probability&lt;/li&gt;
      &lt;li&gt;Generative = P(x and y) = f(x given y) p(y) = p(y given x) f(x) - models joint distribution - more powerful than Discriminative since using more parameters&lt;/li&gt;
      &lt;li&gt;Estimating a density is difficult and need a lot of data leading to high variance and hence Generative model will have bad performance than Discriminative.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;kNN
    &lt;ul&gt;
      &lt;li&gt;circle concept that is used to decide the class of the test point&lt;/li&gt;
      &lt;li&gt;Probabilistic interpretation - (y)&lt;/li&gt;
      &lt;li&gt;Discriminative model&lt;/li&gt;
      &lt;li&gt;Bias - variance tradeoff&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 19 Mar 2017 17:15:34 +0530</pubDate>
        <link>http://localhost:4000/2017/03/notes-for-youtube-ml-mathematicalmonk/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/notes-for-youtube-ml-mathematicalmonk/</guid>
        
        <category>notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Segmentation and Clustering!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Process of Segmentation&lt;/li&gt;
  &lt;li&gt;Standardisation where we treat a group as a whole&lt;/li&gt;
  &lt;li&gt;Localisation where we treat them as individuals&lt;/li&gt;
  &lt;li&gt;Grouping or binning&lt;/li&gt;
  &lt;li&gt;It becomes difficult to group as the number of variables increase&lt;/li&gt;
  &lt;li&gt;Clustering is a mathematical procedure for multidimensional analysis&lt;/li&gt;
  &lt;li&gt;For a given set of objects, this procedure groups similar objects into clusters&lt;/li&gt;
  &lt;li&gt;Distance is way to measure similarity&lt;/li&gt;
  &lt;li&gt;Intra-cluster separation is minimised and inter-cluster distance is maximised&lt;/li&gt;
  &lt;li&gt;eg. Fraudulent insurance Claim, User Segmentation, Bioinformatics Study, Education&lt;/li&gt;
  &lt;li&gt;Supervised and Unsupervised Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 2:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Selecting data based on objectives&lt;/li&gt;
  &lt;li&gt;Use demographic and social data in first phases and remove historical data.&lt;/li&gt;
  &lt;li&gt;Predetermined Bias in Transactional Data - A/B testing&lt;/li&gt;
  &lt;li&gt;Data types in clustering
    &lt;ul&gt;
      &lt;li&gt;Continuous or Ordinal Variables&lt;/li&gt;
      &lt;li&gt;Outliers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Scaling data because distance is important in cluster analysis
    &lt;ul&gt;
      &lt;li&gt;Z score or standard score i.e. no. of SD away from the mean&lt;/li&gt;
      &lt;li&gt;Unit interval compressing all values between 0 and 1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transforming Variables and visualising the data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 3:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Variable Reduction Procedures help in accounting for most of the variables 
 in all of the observed variables&lt;/li&gt;
  &lt;li&gt;Principle variables or artificial variables explain most of the variance&lt;/li&gt;
  &lt;li&gt;Try to use variable reduction when variables are somewhat related&lt;/li&gt;
  &lt;li&gt;Need to weight benefits one can get from reducing the no. of variables used in 
 clustering analysis, with the all important aspect of needing to be able to explain
 the basis for clusters&lt;/li&gt;
  &lt;li&gt;Types of Variable Reduction Procedures
    &lt;ul&gt;
      &lt;li&gt;Factor Analysis - Correlation between variables, trying to figure out causes&lt;/li&gt;
      &lt;li&gt;Principal Component Analysis - accounts for total variation, trying to summarise&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Factor Analysis makes assumptions for underlying causal model while PCA doesn’t&lt;/li&gt;
  &lt;li&gt;In PCA, combine variables that are related to a common story&lt;/li&gt;
  &lt;li&gt;Loadings and Values
    &lt;ul&gt;
      &lt;li&gt;Close to zero loading has less value on components&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 19 Jan 2017 17:15:34 +0530</pubDate>
        <link>http://localhost:4000/2017/01/notes-for-segmentation-and-clustering/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/01/notes-for-segmentation-and-clustering/</guid>
        
        <category>notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Model Building And Validation!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Example of model building
    &lt;ul&gt;
      &lt;li&gt;Toxic and non toxic classification&lt;/li&gt;
      &lt;li&gt;Advertising - collaborative filtering&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;QMV iterative process of analysis
    &lt;ul&gt;
      &lt;li&gt;Questioning&lt;/li&gt;
      &lt;li&gt;Modeling&lt;/li&gt;
      &lt;li&gt;Validating&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Must choose a metric capturing the phenomenon&lt;/li&gt;
  &lt;li&gt;Classification vs Regression in a scenario&lt;/li&gt;
  &lt;li&gt;Classification - Naive Bayes, Perceptron, Logistic Regression&lt;/li&gt;
  &lt;li&gt;Check if data has locality or spatial properties - kNN&lt;/li&gt;
  &lt;li&gt;Consumer Choice Modeling&lt;/li&gt;
  &lt;li&gt;Inter selection time&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 17 Jan 2017 17:15:34 +0530</pubDate>
        <link>http://localhost:4000/2017/01/notes-for-model-building-and-validation/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/01/notes-for-model-building-and-validation/</guid>
        
        <category>notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Intro to Inferential Statistics!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sample mean = Population Mean&lt;/li&gt;
  &lt;li&gt;Sample Standard Deviation = Population Standard Deviation/sqrt(number of samples) = Standard Error&lt;/li&gt;
  &lt;li&gt;z score indicates how many standard deviation is an element from the mean&lt;/li&gt;
  &lt;li&gt;Distribution of sample means is sampling distribution&lt;/li&gt;
  &lt;li&gt;Point Estimate&lt;/li&gt;
  &lt;li&gt;Mean of Treated Population vs Sample Mean&lt;/li&gt;
  &lt;li&gt;In a normal distribution, 68% of sample falls in one Sample SD and 95% SD fall in two Sample SD&lt;/li&gt;
  &lt;li&gt;2 * Sample SD is margin of error&lt;/li&gt;
  &lt;li&gt;95% Confidence Interval is mean +/- margin of error&lt;/li&gt;
  &lt;li&gt;95% of sample means are within 1.96 Standard Errors from the population mean in a sampling distribution&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 08 Jan 2017 17:15:34 +0530</pubDate>
        <link>http://localhost:4000/2017/01/notes-for-udacity-course-inferential-stats/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/01/notes-for-udacity-course-inferential-stats/</guid>
        
        <category>notes</category>
        
        
      </item>
    
      <item>
        <title>Notes for Intro to Statistics!</title>
        <description>&lt;p&gt;This is the course offered by Sabestian Thrun.&lt;/p&gt;

&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using statistics he shows how we are actually less popular than our friends in expectation. This is actually called Friendship paradox.&lt;/li&gt;
  &lt;li&gt;He tells how statistics plays a main role in converting data to decisions and the fact that a good statistician spends a lot of time looking at the data.&lt;/li&gt;
  &lt;li&gt;There is a discussion about scatter plot, linearity, outliers and noise and describes how bar charts address the issue of noise by merging points into single bar.&lt;/li&gt;
  &lt;li&gt;He introduced the concept of interpolation and random noise which is basically deviations from the linear graph.&lt;/li&gt;
  &lt;li&gt;Moving on to bar charts, he talks about how bar charts pool together groups of data and understand global data trends.&lt;/li&gt;
  &lt;li&gt;Histograms - special case of bar charts for 1-D data. They use frequency of 1 variable and depict the median.&lt;/li&gt;
  &lt;li&gt;Relative data visualization in pie charts where he showed how pie charts are invariant to the number and depict just the ratio.&lt;/li&gt;
  &lt;li&gt;Simpson’s paradox - Paradox which appears in different groups of data but disappears when the groups are combined.&lt;a href=&quot;http://www.unc.edu/~nielsen/soci708/cdocs/Berkeley_admissions_bias.pdf&quot;&gt;(Do read associated UC Berkley Incident)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 2:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;He discusses that probability is moving from cause to data and stats is about moving from data to cause.&lt;/li&gt;
  &lt;li&gt;There is a brief introduction to topic of independent and dependent events&lt;/li&gt;
  &lt;li&gt;Conditional Probability&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Fri, 14 Oct 2016 17:15:34 +0530</pubDate>
        <link>http://localhost:4000/2016/10/notes-for-udacity-course-intro-to-stats/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/10/notes-for-udacity-course-intro-to-stats/</guid>
        
        <category>notes</category>
        
        
      </item>
    
  </channel>
</rss>
