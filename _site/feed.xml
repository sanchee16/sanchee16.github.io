<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sancheeta Kaushal</title>
    <description>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</description>
    <link>http://hitchhiker.ma/</link>
    <atom:link href="http://hitchhiker.ma/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 03 Apr 2017 18:35:15 +0530</pubDate>
    <lastBuildDate>Mon, 03 Apr 2017 18:35:15 +0530</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>Notes for Intro to Data Analysis!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data Analysis Process
    &lt;ul&gt;
      &lt;li&gt;Question&lt;/li&gt;
      &lt;li&gt;Wrangle
        &lt;ul&gt;
          &lt;li&gt;Acquiring Data&lt;/li&gt;
          &lt;li&gt;Cleaning Data&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Explore&lt;/li&gt;
      &lt;li&gt;Draw conclusions&lt;/li&gt;
      &lt;li&gt;Communicate&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The process is iterative and need to go back and forth.&lt;/li&gt;
  &lt;li&gt;Each row of csv can be represented in two ways in python
    &lt;ul&gt;
      &lt;li&gt;As a list&lt;/li&gt;
      &lt;li&gt;As a dictionary&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 24 Mar 2017 06:53:34 +0530</pubDate>
        <link>http://hitchhiker.ma/notes/2017/03/24/notes-for-udacity-intro-to-data-analysis.html</link>
        <guid isPermaLink="true">http://hitchhiker.ma/notes/2017/03/24/notes-for-udacity-intro-to-data-analysis.html</guid>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>Notes for CNN for Visual Recognition!</title>
        <description>&lt;p&gt;Lesson 2:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Image Classification&lt;/li&gt;
  &lt;li&gt;Semantic Gap - Representation of image on computer as numbers&lt;/li&gt;
  &lt;li&gt;Challenges are:
    &lt;ul&gt;
      &lt;li&gt;Viewpoint Variation - Camera rotations lead to changes in brightness.&lt;/li&gt;
      &lt;li&gt;Illumination issues&lt;/li&gt;
      &lt;li&gt;Deformation&lt;/li&gt;
      &lt;li&gt;Occlusion&lt;/li&gt;
      &lt;li&gt;Background Clutter&lt;/li&gt;
      &lt;li&gt;Intraclass Variation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Follow a data-driven approach i.e. Collect dataset of images and labels, use ML algos to train an image 
classifier and evaluate classifier on test images.&lt;/li&gt;
  &lt;li&gt;Nearest Neighbour Classifier - Use Manhattan distance&lt;/li&gt;
  &lt;li&gt;Do more compute at train time but prediction should be constant time computation.&lt;/li&gt;
  &lt;li&gt;Aside - Approximate Nearest Neighbour (FLANN)&lt;/li&gt;
  &lt;li&gt;Hyper-parameter - Distance, and the value of k for kNN&lt;/li&gt;
  &lt;li&gt;Training data, validation data and training data set or Cross Validation&lt;/li&gt;
  &lt;li&gt;Linear Classification&lt;/li&gt;
  &lt;li&gt;NN can see, hear, translate, control and think.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lecture 4:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Forward pass gives loss and backward pass gives gradients&lt;/li&gt;
  &lt;li&gt;Gradient Descent
    &lt;ul&gt;
      &lt;li&gt;Numerical which is slow and approx but easy to write&lt;/li&gt;
      &lt;li&gt;Analytical which is fast and exact but error prone&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Computational Graph is huge for Neural Turing Machine&lt;/li&gt;
  &lt;li&gt;Chain rule for backprop&lt;/li&gt;
  &lt;li&gt;Local gradients are computed at the time of forward pass and can be chained to global gradient later at the time of backprop.&lt;/li&gt;
  &lt;li&gt;For plus gate, during back prop, the value for the next gate is 1 * the previous value.&lt;/li&gt;
  &lt;li&gt;For multiplicative gate, during back prop, the value for the next gate is the value of other input * the previous value.&lt;/li&gt;
  &lt;li&gt;Hence, add gate is gradient distributor, the max gate is gradient router and mul gate can be the gradient switcher.&lt;/li&gt;
  &lt;li&gt;At branches, gradients are added according to multivariate chain rule.&lt;/li&gt;
  &lt;li&gt;Graph class with nodes topologically sorted and forward and backward function&lt;/li&gt;
  &lt;li&gt;Lot of memory required to store intermediate results that will be used during back prop&lt;/li&gt;
  &lt;li&gt;For vectors, we have jacobian matrix which stores derivative of each element of output wrt input&lt;/li&gt;
  &lt;li&gt;Vectorized Operations&lt;/li&gt;
  &lt;li&gt;Jacobian matrix is not always a full matrix and is a sparse matrix because there are values only on the diagonal and even not all those values are be used.&lt;/li&gt;
  &lt;li&gt;Backpropagation is recursive application of chain rule along computational graph to computer gradients  of all inputs/params/intermediates&lt;/li&gt;
  &lt;li&gt;Biological description of neurons
    &lt;ul&gt;
      &lt;li&gt;Soma: cell body&lt;/li&gt;
      &lt;li&gt;Dendrites: listeners/input&lt;/li&gt;
      &lt;li&gt;Axon: terminals/output&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Activation functions
    &lt;ul&gt;
      &lt;li&gt;Sigmoid&lt;/li&gt;
      &lt;li&gt;tanh&lt;/li&gt;
      &lt;li&gt;ReLU&lt;/li&gt;
      &lt;li&gt;Maxout&lt;/li&gt;
      &lt;li&gt;Leaky ReLU&lt;/li&gt;
      &lt;li&gt;ELU&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Fully connected layer and hidden layers&lt;/li&gt;
  &lt;li&gt;Kernel trick changes data representation to a space where itâ€™s linearly separable&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lecture 5:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;People hardly train CNN from scratch. Pretraining and fine tuning is used.&lt;/li&gt;
  &lt;li&gt;Transfer Learning
    &lt;ul&gt;
      &lt;li&gt;Train network on Imagenet&lt;/li&gt;
      &lt;li&gt;If small dataset, fix all weights retrain only the classifier on your data. Here CNN is treated as fixed feature extractor.&lt;/li&gt;
      &lt;li&gt;If medium sized dataset, use old weights as initialization, train the full network or only some of the higher layers. eg. Caffe Model Zoo&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;History
    &lt;ul&gt;
      &lt;li&gt;Perceptron&lt;/li&gt;
      &lt;li&gt;Multilayer Perceptron&lt;/li&gt;
      &lt;li&gt;Back Propagation Rules&lt;/li&gt;
      &lt;li&gt;Reinvigorated Research in Deep Learning&lt;/li&gt;
      &lt;li&gt;Imagenet Classification With Deep CNN
        &lt;ul&gt;
          &lt;li&gt;Better ways of Initialization&lt;/li&gt;
          &lt;li&gt;GPUs&lt;/li&gt;
          &lt;li&gt;More data&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Activation Function
    &lt;ul&gt;
      &lt;li&gt;Sigmoid
        &lt;ul&gt;
          &lt;li&gt;Squashing Function to [0, 1]&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 06 Mar 2017 17:30:12 +0530</pubDate>
        <link>http://hitchhiker.ma/notes/2017/03/06/notes-for-CNN-for-visual-recognition.html</link>
        <guid isPermaLink="true">http://hitchhiker.ma/notes/2017/03/06/notes-for-CNN-for-visual-recognition.html</guid>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>Notes for Segmentation and Clustering!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Process of Segmentation&lt;/li&gt;
  &lt;li&gt;Standardisation where we treat a group as a whole&lt;/li&gt;
  &lt;li&gt;Localisation where we treat them as individuals&lt;/li&gt;
  &lt;li&gt;Grouping or binning&lt;/li&gt;
  &lt;li&gt;It becomes difficult to group as the number of variables increase&lt;/li&gt;
  &lt;li&gt;Clustering is a mathematical procedure for multidimensional analysis&lt;/li&gt;
  &lt;li&gt;For a given set of objects, this procedure groups similar objects into clusters&lt;/li&gt;
  &lt;li&gt;Distance is way to measure similarity&lt;/li&gt;
  &lt;li&gt;Intra-cluster separation is minimised and inter-cluster distance is maximised&lt;/li&gt;
  &lt;li&gt;eg. Fraudulent insurance Claim, User Segmentation, Bioinformatics Study, Education&lt;/li&gt;
  &lt;li&gt;Supervised and Unsupervised Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 2:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Selecting data based on objectives&lt;/li&gt;
  &lt;li&gt;Use demographic and social data in first phases and remove historical data.&lt;/li&gt;
  &lt;li&gt;Predetermined Bias in Transactional Data - A/B testing&lt;/li&gt;
  &lt;li&gt;Data types in clustering
    &lt;ul&gt;
      &lt;li&gt;Continuous or Ordinal Variables&lt;/li&gt;
      &lt;li&gt;Outliers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Scaling data because distance is important in cluster analysis
    &lt;ul&gt;
      &lt;li&gt;Z score or standard score i.e. no. of SD away from the mean&lt;/li&gt;
      &lt;li&gt;Unit interval compressing all values between 0 and 1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transforming Variables and visualising the data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 3:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Variable Reduction Procedures help in accounting for most of the variables 
 in all of the observed variables&lt;/li&gt;
  &lt;li&gt;Principle variables or artificial variables explain most of the variance&lt;/li&gt;
  &lt;li&gt;Try to use variable reduction when variables are somewhat related&lt;/li&gt;
  &lt;li&gt;Need to weight benefits one can get from reducing the no. of variables used in 
 clustering analysis, with the all important aspect of needing to be able to explain
 the basis for clusters&lt;/li&gt;
  &lt;li&gt;Types of Variable Reduction Procedures
    &lt;ul&gt;
      &lt;li&gt;Factor Analysis - Correlation between variables, trying to figure out causes&lt;/li&gt;
      &lt;li&gt;Principal Component Analysis - accounts for total variation, trying to summarise&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Factor Analysis makes assumptions for underlying causal model while PCA doesnâ€™t&lt;/li&gt;
  &lt;li&gt;In PCA, combine variables that are related to a common story&lt;/li&gt;
  &lt;li&gt;Loadings and Values
    &lt;ul&gt;
      &lt;li&gt;Close to zero loading has less value on components&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 19 Jan 2017 17:15:34 +0530</pubDate>
        <link>http://hitchhiker.ma/notes/2017/01/19/notes-for-segmentation-and-clustering.html</link>
        <guid isPermaLink="true">http://hitchhiker.ma/notes/2017/01/19/notes-for-segmentation-and-clustering.html</guid>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>Notes for Model Building And Validation!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Example of model building
    &lt;ul&gt;
      &lt;li&gt;Toxic and non toxic classification&lt;/li&gt;
      &lt;li&gt;Advertising - collaborative filtering&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;QMV iterative process of analysis
    &lt;ul&gt;
      &lt;li&gt;Questioning&lt;/li&gt;
      &lt;li&gt;Modeling&lt;/li&gt;
      &lt;li&gt;Validating&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Must choose a metric capturing the phenomenon&lt;/li&gt;
  &lt;li&gt;Classification vs Regression in a scenario&lt;/li&gt;
  &lt;li&gt;Classification - Naive Bayes, Perceptron, Logistic Regression&lt;/li&gt;
  &lt;li&gt;Check if data has locality or spatial properties - kNN&lt;/li&gt;
  &lt;li&gt;Consumer Choice Modeling&lt;/li&gt;
  &lt;li&gt;Inter selection time&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 17 Jan 2017 17:15:34 +0530</pubDate>
        <link>http://hitchhiker.ma/notes/2017/01/17/notes-for-model-building-and-validation.html</link>
        <guid isPermaLink="true">http://hitchhiker.ma/notes/2017/01/17/notes-for-model-building-and-validation.html</guid>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>Notes for Intro to Inferential Statistics!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sample mean = Population Mean&lt;/li&gt;
  &lt;li&gt;Sample Standard Deviation = Population Standard Deviation/sqrt(number of samples) = Standard Error&lt;/li&gt;
  &lt;li&gt;z score indicates how many standard deviation is an element from the mean&lt;/li&gt;
  &lt;li&gt;Distribution of sample means is sampling distribution&lt;/li&gt;
  &lt;li&gt;Point Estimate&lt;/li&gt;
  &lt;li&gt;Mean of Treated Population vs Sample Mean&lt;/li&gt;
  &lt;li&gt;In a normal distribution, 68% of sample falls in one Sample SD and 95% SD fall in two Sample SD&lt;/li&gt;
  &lt;li&gt;2 * Sample SD is margin of error&lt;/li&gt;
  &lt;li&gt;95% Confidence Interval is mean +/- margin of error&lt;/li&gt;
  &lt;li&gt;95% of sample means are within 1.96 Standard Errors from the population mean in a sampling distribution&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 08 Jan 2017 17:15:34 +0530</pubDate>
        <link>http://hitchhiker.ma/notes/2017/01/08/notes-for-udacity-course-inferential-stats.html</link>
        <guid isPermaLink="true">http://hitchhiker.ma/notes/2017/01/08/notes-for-udacity-course-inferential-stats.html</guid>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>Notes for ML!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ML is algorithms for inferring unknowns from knowns. eg. Filtering out spam, Detect Handwriting, Face Detection, Speech Recognition etc.&lt;/li&gt;
  &lt;li&gt;Supervised Learning is given data (x1, y1), (x2, y2), (x3, y3), â€¦â€¦â€¦â€¦.  (xn, yn) , choose a function f(x) = y where xi = data point and yi = class/value and then generalise for new values of x.&lt;/li&gt;
  &lt;li&gt;Two types of Supervised Learning problems
    &lt;ul&gt;
      &lt;li&gt;Classification - yi is from some finite set.&lt;/li&gt;
      &lt;li&gt;Regression - yi are from Real values.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unsupervised Learning is given data (x1, x2, x3,â€¦â€¦..xn)
    &lt;ul&gt;
      &lt;li&gt;Custering&lt;/li&gt;
      &lt;li&gt;Density Estimation&lt;/li&gt;
      &lt;li&gt;Dimensionality Reduction should project it down preserving the structure of data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Variations on supervised and unsupervised learning
    &lt;ul&gt;
      &lt;li&gt;Semi Supervised Learning - (x1, y1), (x2, y2), (x3, y3), â€¦â€¦â€¦â€¦.  (xk, yk), xk+1, xk+2 â€¦.xn,
  predict yk+1, yk+2 â€¦.. yn. eg.&lt;/li&gt;
      &lt;li&gt;Active Learning&lt;/li&gt;
      &lt;li&gt;Decision Theory&lt;/li&gt;
      &lt;li&gt;Reinforcement Learning - maximize overall reward and minimize overall losses.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generative vs Discriminative Models
    &lt;ul&gt;
      &lt;li&gt;Discriminative = P(y given x) which is Conditional Probability&lt;/li&gt;
      &lt;li&gt;Generative = P(x and y) = f(x given y) p(y) = p(y given x) f(x) - models joint distribution - more powerful than Discriminative since using more parameters&lt;/li&gt;
      &lt;li&gt;Estimating a density is difficult and need a lot of data leading to high variance and hence Generative model will have bad performance than Discriminative.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;kNN
    &lt;ul&gt;
      &lt;li&gt;circle concept that is used to decide the class of the test point&lt;/li&gt;
      &lt;li&gt;Probabilistic interpretation - (y)&lt;/li&gt;
      &lt;li&gt;Discriminative model&lt;/li&gt;
      &lt;li&gt;Bias - variance tradeoff&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 19 Dec 2016 17:15:34 +0530</pubDate>
        <link>http://hitchhiker.ma/notes/2016/12/19/notes-for-youtube-ml-mathematicalmonk.html</link>
        <guid isPermaLink="true">http://hitchhiker.ma/notes/2016/12/19/notes-for-youtube-ml-mathematicalmonk.html</guid>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>Notes for Artificial Neural Networks!</title>
        <description>&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;He defines the terminologies as weights denoted by w, bias denoted by b, activation function denoted by g(.), neuron preactivation(I/P) denoted by x, neuron activation(O/P) denoted by h(x). Everything exept b is a vector.&lt;/li&gt;
  &lt;li&gt;He starts a discussion about potential activation functions in ANN.
    &lt;ul&gt;
      &lt;li&gt;Linear Activation Function, g(a) = a, No squashing of I/P
 	- Sigmoid Function, g(a) = 1/(1+e^(-a)), Lies between 0 and 1, always +ve, bounded, strictly increasing.
 	- Tanh Function, g(a) = (e^(2a)-1)/(e^(2a)+1), Lies netween -1 and 1, bounded, strictly increasing
 	- ReLU Function, g(a) = max(0, a), Bounded below 0, Strictly increasing, tends to give neurons with sparse activities because it is 0 over a large range of -ve numbers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;He further discusses the complexity of computations the neural network can perform.&lt;/li&gt;
  &lt;li&gt;He talks about linearly separable problems(binary classification problems) which have a decision boundary and use single neuron to perform logistic regression. This can be achieved using sigmoid.&lt;/li&gt;
  &lt;li&gt;He goes further and talks about alternate representations of input vector which can convert the non lineraly separable problem into a separable one. For eg. XOR can be represented by AND(X^, Y) on x axis and AND(X, Y^) on y axis. He tries to break the problem into pieces and check if single piece of problem can be represented by a neuron or a group of neurons and then combine all the representations to generate a representation for non linearly separable problem.&lt;/li&gt;
  &lt;li&gt;He discusses the hidden layer pre-activation, hidden layer activation and output layer activation for a single layer neural network. Going ahead with multilayer neural network, the only difference is that multilayer neural network has multiple hidden layers.
    &lt;ul&gt;
      &lt;li&gt;To perform binary classification, one can use sigmoid as output activation function.&lt;/li&gt;
      &lt;li&gt;For multiclass classification one needs multiple outputs, so one uses conditional probablity 
  p(y = c|x) where c is the class and then using softmax activation function; A function which sums to 1 when all the probablities are added. Additionally, softmax is strictly positive.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;He discusses universal approximation throem i.e. a single layer hidden neural network with linear output unit which can approx. any continuous function arbitrarily well, given enough hidden units.&lt;/li&gt;
  &lt;li&gt;He discusses the inspiration that neural networks draw from biological neurons and talks about the idea of different layers and how they work in parallel with the visual cortex. He uses edges and points to introduce the idea of complex shapes formed using these simple structures and uses face detection as an example. He defines the following terms too.
    &lt;ul&gt;
      &lt;li&gt;Action potential is an electrical impulse that travels through the axon.&lt;/li&gt;
      &lt;li&gt;Firing rate of a neuron is frequency with which a neuron can spike.&lt;/li&gt;
      &lt;li&gt;Neurons tend to excite or inhibit other neurons.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Analogy between artificial NN and biological NN
    &lt;ul&gt;
      &lt;li&gt;Activation function is analogous to firing rate.&lt;/li&gt;
      &lt;li&gt;Weights in ANN decide the excitation or inhibition of a neuron.&lt;/li&gt;
      &lt;li&gt;Activation function and bias model the action potential of a neuron.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 21 Oct 2016 17:15:34 +0530</pubDate>
        <link>http://hitchhiker.ma/notes/2016/10/21/notes-for-youtube-course-neural-networks.html</link>
        <guid isPermaLink="true">http://hitchhiker.ma/notes/2016/10/21/notes-for-youtube-course-neural-networks.html</guid>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>Notes for Intro to Statistics!</title>
        <description>&lt;p&gt;This is the course offered by Sabestian Thrun.&lt;/p&gt;

&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using statistics he shows how we are actually less popular than our friends in expectation. This is actually called Friendship paradox.&lt;/li&gt;
  &lt;li&gt;He tells how statistics plays a main role in converting data to decisions and the fact that a good statistician spends a lot of time looking at the data.&lt;/li&gt;
  &lt;li&gt;There is a discussion about scatter plot, linearity, outliers and noise and describes how bar charts address the issue of noise by merging points into single bar.&lt;/li&gt;
  &lt;li&gt;He introduced the concept of interpolation and random noise which is basically deviations from the linear graph.&lt;/li&gt;
  &lt;li&gt;Moving on to bar charts, he talks about how bar charts pool together groups of data and understand global data trends.&lt;/li&gt;
  &lt;li&gt;Histograms - special case of bar charts for 1-D data. They use frequency of 1 variable and depict the median.&lt;/li&gt;
  &lt;li&gt;Relative data visualization in pie charts where he showed how pie charts are invariant to the number and depict just the ratio.&lt;/li&gt;
  &lt;li&gt;Simpsonâ€™s paradox - Paradox which appears in different groups of data but disappears when the groups are combined.&lt;a href=&quot;http://www.unc.edu/~nielsen/soci708/cdocs/Berkeley_admissions_bias.pdf&quot;&gt;(Do read associated UC Berkley Incident)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lesson 2:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;He discusses that probability is moving from cause to data and stats is about moving from data to cause.&lt;/li&gt;
  &lt;li&gt;There is a brief introduction to topic of independent and dependent events&lt;/li&gt;
  &lt;li&gt;Conditional Probability&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Fri, 14 Oct 2016 17:15:34 +0530</pubDate>
        <link>http://hitchhiker.ma/notes/2016/10/14/notes-for-udacity-course-intro-to-stats.html</link>
        <guid isPermaLink="true">http://hitchhiker.ma/notes/2016/10/14/notes-for-udacity-course-intro-to-stats.html</guid>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>Notes for Neural Networks!</title>
        <description>&lt;p&gt;This is the course offered by Jeffery Hilton.&lt;/p&gt;

&lt;p&gt;Week 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;He starts with a dicussion about the need for machine learning models and talks about the situations where it is really hard to write programs and form rule based solutions since there is no single answer. Additionally, rules need to change and outsmart the ones who try to break the system.&lt;/li&gt;
  &lt;li&gt;Approach is to take examples and train the system using these inputs and outputs. Machine Learning algorithms take care of generalisation.&lt;/li&gt;
  &lt;li&gt;Learning algorithms are really good solutions for
    &lt;ul&gt;
      &lt;li&gt;Recognising Patterns - Recognising facial expressions and identities.&lt;/li&gt;
      &lt;li&gt;Recognising Anomalies - Unusual Sequence of Credit Card Transactions.&lt;/li&gt;
      &lt;li&gt;Prediction - Future Stock Prices.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Discussion about MNIST databased of Handwritten Digits, ImageNet competition and Speech Recognition Task.&lt;/li&gt;
  &lt;li&gt;For speech recognition task, we have pre-processing, the acoustic model and decoding as various steps.&lt;/li&gt;
  &lt;li&gt;Talked about real neurons and how they insipred the development of neural networks as a field of machine learning.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 14 Oct 2016 17:15:34 +0530</pubDate>
        <link>http://hitchhiker.ma/notes/2016/10/14/notes-for-coursera-course-neural-networks.html</link>
        <guid isPermaLink="true">http://hitchhiker.ma/notes/2016/10/14/notes-for-coursera-course-neural-networks.html</guid>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>Notes for Classification Model!</title>
        <description>&lt;p&gt;This is first in the series of notes that I plan to put online which I take during my online courses. This one specifically is from the Udacity Course Classification Models.&lt;/p&gt;

&lt;p&gt;The course start with a discussion around a problem where we need to predict which things people are more likely to buy with the change of weather.&lt;/p&gt;

&lt;p&gt;Lesson 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Discussion about examples of classification problems. For eg. determining if a soyabean has a disease or not using an image.&lt;/li&gt;
  &lt;li&gt;Discussion about binary(either of the two values) and non-binary(categorical variables) examples.&lt;/li&gt;
  &lt;li&gt;Discussion about Statistical Terms - Target and Predictor Variable.&lt;/li&gt;
  &lt;li&gt;Target Variable - Field we are trying to understand and predict - Dependent Variable.&lt;/li&gt;
  &lt;li&gt;Predictor Variable - Used to predict the target variable - Independent Variable.&lt;/li&gt;
  &lt;li&gt;Remove Duplicate Variables(one variable subset of other) identifying correlations.&lt;/li&gt;
  &lt;li&gt;Correlation - Measure of association between two variables whose vaues lie between -1 to 1.&lt;/li&gt;
  &lt;li&gt;Three types of associations to be studied for understanding correlation
    &lt;ul&gt;
      &lt;li&gt;Pearson correlation&lt;/li&gt;
      &lt;li&gt;Spearmanâ€™s Rank correlation&lt;/li&gt;
      &lt;li&gt;Hoeffdingâ€™s Independent Test&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pearson Correlation - Correlation Plots
    &lt;ul&gt;
      &lt;li&gt;No issues during training but issues while testing or predicting.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Key take outs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For those who are already aware of Machine Learning, Predictor Variable is what we call as a feature and Target Variable is the objective function.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 13 Oct 2016 06:53:34 +0530</pubDate>
        <link>http://hitchhiker.ma/notes/2016/10/13/notes-for-udacity-course-classification-models.html</link>
        <guid isPermaLink="true">http://hitchhiker.ma/notes/2016/10/13/notes-for-udacity-course-classification-models.html</guid>
        
        
        <category>notes</category>
        
      </item>
    
  </channel>
</rss>
